<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>Module 4: Text Analysis and Entity Resolution Lab - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/img/favicon.ico"/>
<script>window.settings = {"enableSshKeyUI":true,"enableOnDemandClusterType":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","workspaceFeaturedLinks":[{"linkURI":"https://docs.databricks.com/index.html","displayName":"Documentation","icon":"question"},{"linkURI":"https://docs.databricks.com/release-notes/product/latest.html","displayName":"Release Notes","icon":"code"},{"linkURI":"https://docs.databricks.com/spark/latest/training/index.html","displayName":"Training & Tutorials","icon":"graduation-cap"}],"enableClearStateFeature":false,"dbcForumURL":"http://forums.databricks.com/","enableProtoClusterInfoDeltaPublisher":false,"maxCustomTags":45,"enableInstanceProfilesUIInJobs":true,"nodeInfo":{"node_types":[{"support_ssh":false,"spark_heap_memory":4800,"instance_type_id":"r3.2xlarge","spark_core_oversubscription_factor":8.0,"node_type_id":"dev-tier-node","description":"Community Optimized","support_cluster_tags":false,"container_memory_mb":6000,"memory_mb":6144,"category":"Community Edition","num_cores":0.88,"support_ebs_volumes":false}],"default_node_type_id":"dev-tier-node"},"enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":999999,"enableTableHandler":true,"maxEbsVolumesPerInstance":10,"isAdmin":true,"deltaProcessingBatchSize":1000,"enableLargeResultDownload":true,"zoneInfos":[{"id":"us-west-2c","isDefault":true},{"id":"us-west-2b","isDefault":false},{"id":"us-west-2a","isDefault":false}],"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":true,"enableMaxConcurrentRuns":true,"enableJobAclsConfig":false,"enableFullTextSearch":false,"enableElasticSparkUI":false,"enableNewClustersCreate":false,"clusters":true,"allowRunOnPendingClusters":true,"applications":false,"fileStoreBase":"FileStore","enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":false,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":false,"enableResetPassword":true,"maxClusterTagValueLength":255,"enableJobsSparkUpgrade":true,"sparkVersions":[{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.0-ubuntu15.10-scala2.10","displayName":"Spark 2.0.0 (Scala 2.10)","packageLabel":"spark-image-073c1b52ace74f251fae2680624a0d8d184a8b57096d1c21c5ce56c29be6a37a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db3-scala2.10","displayName":"Spark 2.0.2-db3 (Scala 2.10)","packageLabel":"spark-image-584091dedb690de20e8cf22d9e02fdcce1281edda99eedb441a418d50e28088f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.11","displayName":"Spark 2.1.0-db1 (Scala 2.11)","packageLabel":"spark-image-e8ad5b72cf0f899dcf2b4720c1f572ab0e87a311d6113b943b4e1d4a7edb77eb","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-gpu-scala2.11","displayName":"Spark 2.0 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-6c2dd678fff350c03ba0e945bab52d0080cd857a39c99a22131b3e824bb8096f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop2-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-eaa8d9b990015a14e032fb2e2e15be0b8d5af9627cd01d855df728b67969d5d9","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db2-scala2.10","displayName":"Spark 2.0.2-db2 (Scala 2.10)","packageLabel":"spark-image-36d48f22cca7a907538e07df71847dd22aaf84a852c2eeea2dcefe24c681602f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.11, deprecated)","packageLabel":"spark-image-8e1c50d626a52eac5a6c8129e09ae206ba9890f4523775f77af4ad6d99a64c44","upgradable":true,"deprecated":true,"customerVisible":true},{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-dd209a93dfbacf46690f2f46369e1df041a4c7f7c51e9e7752f6106dfcca9295","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.11","displayName":"Spark 2.0.2-db1 (Scala 2.11)","packageLabel":"spark-image-c2d623f03dd44097493c01aa54a941fc31978ebe6d759b36c75b716b2ff6ab9c","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-scala2.11","displayName":"Spark 2.0 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-6fb640835bd45a2e2095758663e237aefe80671acacc2e6377eec5ecccb9004b","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.x-scala2.10","displayName":"Spark 2.1 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-e551c0b5347cb980800160ec8d8835ebddad78c02653870e55d7b9e7e94e1188","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db2-scala2.11","displayName":"Spark 2.0.2-db2 (Scala 2.11)","packageLabel":"spark-image-4fa852ba378e97815083b96c9cada7b962a513ec23554a5fc849f7f1dd8c065a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.1-db1-scala2.11","displayName":"Spark 2.0.1-db1 (Scala 2.11)","packageLabel":"spark-image-10ab19f634bbfdb860446c326a9f76dc25bfa87de6403b980566279142a289ea","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db3-scala2.11","displayName":"Spark 2.0.2-db3 (Scala 2.11)","packageLabel":"spark-image-7fd7aaa89d55692e429115ae7eac3b1a1dc4de705d50510995f34306b39c2397","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop1-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-d50af1032799546b8ccbeeb76889a20c819ebc2a0e68ea20920cb30d3895d3ae","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.10","displayName":"Spark 2.0.2-db1 (Scala 2.10)","packageLabel":"spark-image-654bdd6e9bad70079491987d853b4b7abf3b736fff099701501acaabe0e75c41","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.10, deprecated)","packageLabel":"spark-image-a659f3909d51b38d297b20532fc807ecf708cfb7440ce9b090c406ab0c1e4b7e","upgradable":true,"deprecated":true,"customerVisible":true},{"key":"2.0.1-db1-scala2.10","displayName":"Spark 2.0.1-db1 (Scala 2.10)","packageLabel":"spark-image-5a13c2db3091986a4e7363006cc185c5b1108c7761ef5d0218506cf2e6643840","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.x-scala2.11","displayName":"Spark 2.1 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-1edc20eb46270d5686337d5003a109b69ca51b7dde56895e07282397d0a151a2","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.10","displayName":"Spark 2.1.0-db1 (Scala 2.10)","packageLabel":"spark-image-f0ab82a5deb7908e0d159e9af066ba05fb56e1edb35bdad41b7ad2fd62a9b546","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.0-ubuntu15.10-scala2.11","displayName":"Spark 2.0.0 (Scala 2.11)","packageLabel":"spark-image-b4ec141e751f201399f8358a82efee202560f7ed05e1a04a2ae8778f6324b909","upgradable":true,"deprecated":false,"customerVisible":true}],"enableRestrictedClusterCreation":true,"enableFeedback":true,"enableClusterAutoScaling":false,"enableUserVisibleDefaultTags":true,"defaultNumWorkers":0,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":false,"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","driverStdoutFilePrefix":"stdout","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"class-node":1,"p2.8xlarge":16,"r3.8xlarge":8,"dev-tier-node":1,"c3.8xlarge":4,"r3.4xlarge":4,"i2.4xlarge":6,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":6,"memory-optimized":1,"p2.16xlarge":24,"c3.2xlarge":1,"c4.2xlarge":1,"i2.xlarge":1.5,"compute-optimized":1,"c4.4xlarge":2,"c3.4xlarge":2,"g2.2xlarge":1.5,"p2.xlarge":2,"c4.8xlarge":4,"r3.xlarge":1,"i2.8xlarge":12},"enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableEBSVolumesUI":false,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableClusterTagsUIForJobs":true,"enableClusterTagsUI":true,"enableNotebookHistoryDiffing":true,"branch":"2.36","accountsLimit":3,"enableSparkEnvironmentVariables":true,"enableX509Authentication":false,"enableNotebookGitBranching":true,"local":false,"enableClusterAutoScalingForJobs":false,"enableStrongPassword":false,"displayDefaultContainerMemoryGB":6,"disableS3TableImport":false,"deploymentMode":"production","useSpotForWorkers":true,"enableUserInviteWorkflow":true,"enableStaticNotebooks":true,"enableCssTransitions":true,"minClusterTagKeyLength":1,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":false,"useTempS3UrlForTableUpload":false,"notifyLastLogin":false,"enableNotebookGitVersioning":true,"files":"files/","feedbackEmail":"feedback@databricks.com","enableDriverLogsUI":true,"enableWorkspaceAclsConfig":false,"dropzoneMaxFileSize":4096,"enableNewClustersList":false,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"defaultSparkVersion":{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-dd209a93dfbacf46690f2f46369e1df041a4c7f7c51e9e7752f6106dfcca9295","upgradable":true,"deprecated":false,"customerVisible":true},"enableCustomSpotPricing":true,"enableMountAclsConfig":false,"useDevTierHomePage":true,"enablePublishHub":false,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":false,"enableClusterAclsByTier":false,"databricksDocsBaseUrl":"https://docs.databricks.com/","disallowAddingAdmins":true,"enableSparkConfUI":true,"featureTier":"DEVELOPER_BASIC_TIER","enableOrgSwitcherUI":true,"clustersLimit":1,"enableJdbcImport":true,"logfiles":"logfiles/","enableWebappSharding":true,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":false,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"dbcFeedbackURL":"mailto:feedback@databricks.com","enableMountAclService":true,"enableWorkspaceAcls":false,"maxClusterTagKeyLength":127,"gitHash":"c0d7fe236206e0401a272e44686ca66f80deb056","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableMiniClusters":true,"enableDebugUI":false,"allowNonAdminUsers":true,"enableSingleSignOnByTier":false,"enableJobsRetryOnTimeout":true,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/","enableSpotClusterType":true,"enableSparkPackages":true,"dynamicSparkVersions":true,"enableNotebookHistoryUI":true,"showDebugCounters":false,"enableInstanceProfilesUI":true,"enableFolderHtmlExport":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.databricks.com/_static/notebooks/gentle-introduction-to-apache-spark.html","displayName":"Introduction to Apache Spark on Databricks","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/databricks-for-data-scientists.html","displayName":"Databricks for Data Scientists","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/structured-streaming-python.html","displayName":"Introduction to Structured Streaming","icon":"img/home/Python_icon.svg"}],"upgradeURL":"https://accounts.cloud.databricks.com/registration.html#login","notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":false,"enableTerminal":false,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"accounts":true,"useOnDemandClustersByDefault":true,"useFramedStaticNotebooks":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4,"enableNewClustersGet":false,"showSqlProxyUI":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":2216104459508870,"name":"Module 4: Text Analysis and Entity Resolution Lab","language":"python","commands":[{"version":"CommandV1","origId":2216104459508872,"guid":"3027d9b7-d5ba-49c8-8430-d0d68d6ee192","subtype":"command","commandType":"auto","position":2.0,"command":"%md\n<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>.","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482177498E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"87ed2f9f-4618-4cd5-86a7-6ddb4a38556b"},{"version":"CommandV1","origId":2216104459508873,"guid":"07d692b7-47b9-4c3b-9582-a4a944b4d861","subtype":"command","commandType":"auto","position":2.5,"command":"%md\nBefore beginning this lab exercise, please watch the lectures for module four.  They can be found in \"Module 4: Lectures\" notebook.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.48248217752E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"1c340f05-15ae-45b3-951b-c3daf949379f"},{"version":"CommandV1","origId":2216104459508874,"guid":"5a2dec18-2e8f-4c88-bce4-b85a77b1beff","subtype":"command","commandType":"auto","position":3.0,"command":"%md\n#![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n# **Text Analysis and Entity Resolution**\nEntity resolution is a common, yet difficult problem in data cleaning and integration. This lab will demonstrate how we can use Apache Spark to apply powerful and scalable text analysis techniques and perform entity resolution across two datasets of commercial products.","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482177542E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e2d550aa-5789-4bbc-83f8-744abbe9b1a8"},{"version":"CommandV1","origId":2216104459508875,"guid":"0e5f8d0c-5c43-4739-a25a-bd38b04c4fda","subtype":"command","commandType":"auto","position":4.0,"command":"%md\nEntity Resolution, or \"[Record linkage][wiki]\" is the term used by statisticians, epidemiologists, and historians, among others, to describe the process of joining records from one data source with another that describe the same entity. Our terms with the same meaning include, \"entity disambiguation/linking\", duplicate detection\", \"deduplication\", \"record matching\", \"(reference) reconciliation\", \"object identification\", \"data/information integration\", and \"conflation\".\n \nEntity Resolution (ER) refers to the task of finding records in a dataset that refer to the same entity across different data sources (e.g., data files, books, websites, databases). ER is necessary when joining datasets based on entities that may or may not share a common identifier (e.g., database key, URI, National identification number), as may be the case due to differences in record shape, storage location, and/or curator style or preference. A dataset that has undergone ER may be referred to as being cross-linked.\n[wiki]: https://en.wikipedia.org/wiki/Record_linkage","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482177563E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"0191b251-09ba-492b-97a9-c6b13f8c8a18"},{"version":"CommandV1","origId":2216104459508876,"guid":"1c8314a4-4bb4-467e-bdea-2467ea75b203","subtype":"command","commandType":"auto","position":5.0,"command":"labVersion = 'cs100.1x-lab3-1.0.4'","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482482178953E12,"submitTime":1.482482177587E12,"finishTime":1.482482179023E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"73f71b7a-db4b-4715-a95f-290bc99160ef"},{"version":"CommandV1","origId":2216104459508877,"guid":"59ead347-d3ce-43f2-aff5-9923ca129a08","subtype":"command","commandType":"auto","position":6.0,"command":"%md\n#### Code\nThis assignment can be completed using basic Python, pySpark Transformations and actions, and the plotting library matplotlib. Other libraries are not allowed.\n \n#### Files\nData files for this assignment are from the [metric-learning](https://code.google.com/p/metric-learning/) project and can be found at:\n`cs100/lab3`\n \nThe directory contains the following files:\n* **Google.csv**, the Google Products dataset\n* **Amazon.csv**, the Amazon dataset\n* **Google_small.csv**, 200 records sampled from the Google data\n* **Amazon_small.csv**, 200 records sampled from the Amazon data\n* **Amazon_Google_perfectMapping.csv**, the \"gold standard\" mapping\n* **stopwords.txt**, a list of common English words\n \nBesides the complete data files, there are \"sample\" data files for each dataset - we will use these for **Part 1**. In addition, there is a \"gold standard\" file that contains all of the true mappings between entities in the two datasets. Every row in the gold standard file has a pair of record IDs (one Google, one Amazon) that belong to two record that describe the same thing in the real world. We will use the gold standard to evaluate our algorithms.","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482177602E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"68a229f5-d634-433b-8eae-adfb16a3921c"},{"version":"CommandV1","origId":2216104459508878,"guid":"b8093d2d-f1b0-45b4-9f67-e0ce71544f32","subtype":"command","commandType":"auto","position":7.0,"command":"%md\n#### **Part 0: Preliminaries**\nWe read in each of the files and create an RDD consisting of lines.\nFor each of the data files (\"Google.csv\", \"Amazon.csv\", and the samples), we want to parse the IDs out of each record. The IDs are the first column of the file (they are URLs for Google, and alphanumeric strings for Amazon). Omitting the headers, we load these data files into pair RDDs where the *mapping ID* is the key, and the value is a string consisting of the name/title, description, and manufacturer from the record.\n \nThe file format of an Amazon line is:\n \n   `\"id\",\"title\",\"description\",\"manufacturer\",\"price\"`\n \nThe file format of a Google line is:\n \n   `\"id\",\"name\",\"description\",\"manufacturer\",\"price\"`","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482177624E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"62d8798f-fba2-47eb-9f7e-7339e6c82c8d"},{"version":"CommandV1","origId":2216104459508879,"guid":"8533c4fe-1a4b-4efa-89cf-b33d957ce87b","subtype":"command","commandType":"auto","position":8.0,"command":"import re\nDATAFILE_PATTERN = '^(.+),\"(.+)\",(.*),(.*),(.*)'\n\ndef removeQuotes(s):\n    \"\"\" Remove quotation marks from an input string\n    Args:\n        s (str): input string that might have the quote \"\" characters\n    Returns:\n        str: a string without the quote characters\n    \"\"\"\n    return ''.join(i for i in s if i!='\"')\n\n\ndef parseDatafileLine(datafileLine):\n    \"\"\" Parse a line of the data file using the specified regular expression pattern\n    Args:\n        datafileLine (str): input string that is a line from the data file\n    Returns:\n        str: a string parsed using the given regular expression and without the quote characters\n    \"\"\"\n    match = re.search(DATAFILE_PATTERN, datafileLine)\n    if match is None:\n        print 'Invalid datafile line: %s' % datafileLine\n        return (datafileLine, -1)\n    elif match.group(1) == '\"id\"':\n        print 'Header datafile line: %s' % datafileLine\n        return (datafileLine, 0)\n    else:\n        product = '%s %s %s' % (match.group(2), match.group(3), match.group(4))\n        return ((removeQuotes(match.group(1)), product), 1)","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482482179028E12,"submitTime":1.482482177646E12,"finishTime":1.482482179099E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6552ae08-478c-44c7-bece-58bc1a22d615"},{"version":"CommandV1","origId":2216104459508880,"guid":"b2a05bf4-61c1-49cb-819d-817a7c9c93d7","subtype":"command","commandType":"auto","position":9.5,"command":"%md **WARNING:** If *test_helper*, required in the cell below, is not installed, follow the instructions [here](https://databricks-staging-cloudfront.staging.cloud.databricks.com/public/c65da9a2fa40e45a2028cddebe45b54c/8637560089690848/4187311313936645/6977722904629137/05f3c2ecc3.html).","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482177662E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"737aaa73-61c3-4dc9-b3fb-78cc8e579b32"},{"version":"CommandV1","origId":2216104459508881,"guid":"9d659637-106e-4065-9739-6aa4c68a2b28","subtype":"command","commandType":"auto","position":10.0,"command":"import sys\nimport os\nfrom test_helper import Test\n\nbaseDir = os.path.join('databricks-datasets')\ninputPath = os.path.join('cs100', 'lab3', 'data-001')\n\nGOOGLE_PATH = 'Google.csv'\nGOOGLE_SMALL_PATH = 'Google_small.csv'\nAMAZON_PATH = 'Amazon.csv'\nAMAZON_SMALL_PATH = 'Amazon_small.csv'\nGOLD_STANDARD_PATH = 'Amazon_Google_perfectMapping.csv'\nSTOPWORDS_PATH = 'stopwords.txt'\n\ndef parseData(filename):\n    \"\"\" Parse a data file\n    Args:\n        filename (str): input file name of the data file\n    Returns:\n        RDD: a RDD of parsed lines\n    \"\"\"\n    return (sc\n            .textFile(filename, 4, 0)\n            .map(parseDatafileLine))\n\ndef loadData(path):\n    \"\"\" Load a data file\n    Args:\n        path (str): input file name of the data file\n    Returns:\n        RDD: a RDD of parsed valid lines\n    \"\"\"\n    filename = os.path.join(baseDir, inputPath, path)\n    raw = parseData(filename).cache()\n    failed = (raw\n              .filter(lambda s: s[1] == -1)\n              .map(lambda s: s[0]))\n    for line in failed.take(10):\n        print '%s - Invalid datafile line: %s' % (path, line)\n    valid = (raw\n             .filter(lambda s: s[1] == 1)\n             .map(lambda s: s[0])\n             .cache())\n    print '%s - Read %d lines, successfully parsed %d lines, failed to parse %d lines' % (path,\n                                                                                        raw.count(),\n                                                                                        valid.count(),\n                                                                                        failed.count())\n    assert failed.count() == 0\n    assert raw.count() == (valid.count() + 1)\n    return valid\n\ngoogleSmall = loadData(GOOGLE_SMALL_PATH)\ngoogle = loadData(GOOGLE_PATH)\namazonSmall = loadData(AMAZON_SMALL_PATH)\namazon = loadData(AMAZON_PATH)\n","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Google_small.csv - Read 201 lines, successfully parsed 200 lines, failed to parse 0 lines\nGoogle.csv - Read 3227 lines, successfully parsed 3226 lines, failed to parse 0 lines\nAmazon_small.csv - Read 201 lines, successfully parsed 200 lines, failed to parse 0 lines\nAmazon.csv - Read 1364 lines, successfully parsed 1363 lines, failed to parse 0 lines\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482482179104E12,"submitTime":1.482482177685E12,"finishTime":1.482482183851E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d747325a-b4a6-4c9f-b7f3-c67591623d03"},{"version":"CommandV1","origId":2216104459508882,"guid":"3249a606-d412-4c21-b561-c5a6885a9887","subtype":"command","commandType":"auto","position":11.0,"command":"%md\nLet's examine the lines that were just loaded in the two subset (small) files - one from Google and one from Amazon","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482177701E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d2c83a1c-b77e-45b9-b879-e6ad2295c320"},{"version":"CommandV1","origId":2216104459508883,"guid":"d2f590ff-c0f6-462f-a3d5-03786a4c1442","subtype":"command","commandType":"auto","position":12.0,"command":"for line in googleSmall.take(3):\n    print 'google: %s: %s\\n' % (line[0], line[1])\n\nfor line in amazonSmall.take(3):\n    print 'amazon: %s: %s\\n' % (line[0], line[1])","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">google: http://www.google.com/base/feeds/snippets/11448761432933644608: spanish vocabulary builder &quot;expand your vocabulary! contains fun lessons that both teach and entertain you&apos;ll quickly find yourself mastering new terms. includes games and more!&quot; \n\ngoogle: http://www.google.com/base/feeds/snippets/8175198959985911471: topics presents: museums of world &quot;5 cd-rom set. step behind the velvet rope to examine some of the most treasured collections of antiquities art and inventions. includes the following the louvre - virtual visit 25 rooms in full screen interactive video detailed map of the louvre ...&quot; \n\ngoogle: http://www.google.com/base/feeds/snippets/18445827127704822533: sierrahome hse hallmark card studio special edition win 98 me 2000 xp &quot;hallmark card studio special edition (win 98 me 2000 xp)&quot; &quot;sierrahome&quot;\n\namazon: b000jz4hqo: clickart 950 000 - premier image pack (dvd-rom)  &quot;broderbund&quot;\n\namazon: b0006zf55o: ca international - arcserve lap/desktop oem 30pk &quot;oem arcserve backup v11.1 win 30u for laptops and desktops&quot; &quot;computer associates&quot;\n\namazon: b00004tkvy: noah&apos;s ark activity center (jewel case ages 3-8)  &quot;victory multimedia&quot;\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482482183857E12,"submitTime":1.482482177722E12,"finishTime":1.48248218403E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"3adcc396-af2a-49eb-be0e-4381f4b30933"},{"version":"CommandV1","origId":2216104459508884,"guid":"63aa464f-5cd2-4993-8241-f72d39f0ad11","subtype":"command","commandType":"auto","position":13.0,"command":"%md\n#### **Part 1: ER as Text Similarity - Bags of Words**\n \nA simple approach to entity resolution is to treat all records as strings and compute their similarity with a string distance function. In this part, we will build some components for performing bag-of-words text-analysis, and then use them to compute record similarity.\n[Bag-of-words][bag-of-words] is a conceptually simple yet powerful approach to text analysis.\n \nThe idea is to treat strings, a.k.a. **documents**, as *unordered collections* of words, or **tokens**, i.e., as bags of words.\n> **Note on terminology**: a \"token\" is the result of parsing the document down to the elements we consider \"atomic\" for the task at hand.  Tokens can be things like words, numbers, acronyms, or other exotica like word-roots or fixed-length character strings.\n> Bag of words techniques all apply to any sort of token, so when we say \"bag-of-words\" we really mean \"bag-of-tokens,\" strictly speaking.\nTokens become the atomic unit of text comparison. If we want to compare two documents, we count how many tokens they share in common. If we want to search for documents with keyword queries (this is what Google does), then we turn the keywords into tokens and find documents that contain them. The power of this approach is that it makes string comparisons insensitive to small differences that probably do not affect meaning much, for example, punctuation and word order.\n[bag-of-words]: https://en.wikipedia.org/wiki/Bag-of-words_model","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482177738E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"cbd1e328-d0f4-4bdf-b28b-6b9f824691ef"},{"version":"CommandV1","origId":2216104459508885,"guid":"a4bc65b2-ed88-4b46-b9db-57c878f7ddcf","subtype":"command","commandType":"auto","position":14.0,"command":"%md\n#### **1(a) Tokenize a String**\nImplement the function `simpleTokenize(string)` that takes a string and returns a list of non-empty tokens in the string. `simpleTokenize` should split strings using the provided regular expression. Since we want to make token-matching case insensitive, make sure all tokens are turned lower-case. Give an interpretation, in natural language, of what the regular expression, `split_regex`, matches.\nIf you need help with Regular Expressions, try the site [regex101](https://regex101.com/) where you can interactively explore the results of applying different regular expressions to strings. *Note that \\W includes the \"_\" character*.  You should use [re.split()](https://docs.python.org/2/library/re.html#re.split) to perform the string split. Also, make sure you remove any empty tokens.","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482177759E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"850cd126-a735-463a-9344-c215b2ac54ca"},{"version":"CommandV1","origId":2216104459508886,"guid":"141101c9-984e-4e0d-91d8-92244389804d","subtype":"command","commandType":"auto","position":15.0,"command":"# TODO: Replace <FILL IN> with appropriate code\nquickbrownfox = 'A quick brown fox jumps over the lazy dog.'\nsplit_regex = r'\\W+'\n\ndef simpleTokenize(string):\n    \"\"\" A simple implementation of input string tokenization\n    Args:\n        string (str): input string\n    Returns:\n        list: a list of tokens\n    \"\"\"\n    return [x for x in re.split('\\W+', string.lower()) if x!='']\n\nprint simpleTokenize(quickbrownfox) # Should give ['a', 'quick', 'brown', ... ]","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">[&apos;a&apos;, &apos;quick&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;jumps&apos;, &apos;over&apos;, &apos;the&apos;, &apos;lazy&apos;, &apos;dog&apos;]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482482184035E12,"submitTime":1.48248217778E12,"finishTime":1.482482184064E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"134b29ac-694c-48b9-adc4-67c06c306bea"},{"version":"CommandV1","origId":2216104459508887,"guid":"07cdeec6-2aa2-4531-b4a1-f6e928d17c15","subtype":"command","commandType":"auto","position":16.0,"command":"# TEST Tokenize a String (1a)\nTest.assertEquals(simpleTokenize(quickbrownfox),\n                  ['a','quick','brown','fox','jumps','over','the','lazy','dog'],\n                  'simpleTokenize should handle sample text')\nTest.assertEquals(simpleTokenize(' '), [], 'simpleTokenize should handle empty string')\nTest.assertEquals(simpleTokenize('!!!!123A/456_B/789C.123A'), ['123a','456_b','789c','123a'],\n                  'simpleTokenize should handle punctuations and lowercase result')\nTest.assertEquals(simpleTokenize('fox fox'), ['fox', 'fox'],\n                  'simpleTokenize should not remove duplicates')","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">1 test passed.\n1 test passed.\n1 test passed.\n1 test passed.\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482482184069E12,"submitTime":1.482482177795E12,"finishTime":1.482482184109E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"97dc4ec1-9d08-4929-a932-438df42e1013"},{"version":"CommandV1","origId":2216104459508888,"guid":"5f5a977b-dc95-49c3-bca7-05d0471c18b1","subtype":"command","commandType":"auto","position":17.0,"command":"%md\n#### **(1b) Removing stopwords**\n*[Stopwords][stopwords]* are common (English) words that do not contribute much to the content or meaning of a document (e.g., \"the\", \"a\", \"is\", \"to\", etc.). Stopwords add noise to bag-of-words comparisons, so they are usually excluded.\nUsing the included file \"stopwords.txt\", implement `tokenize`, an improved tokenizer that does not emit stopwords.\n[stopwords]: https://en.wikipedia.org/wiki/Stop_words","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.48248217781E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ca57d7fb-67d6-4e72-90d3-3c1af0557431"},{"version":"CommandV1","origId":2216104459508889,"guid":"58fbcea3-72c8-435b-8698-fac8f4353e92","subtype":"command","commandType":"auto","position":18.0,"command":"# TODO: Replace <FILL IN> with appropriate code\nstopfile = os.path.join(baseDir, inputPath, STOPWORDS_PATH)\nstopwords = set(sc.textFile(stopfile).collect())\nprint 'These are the stopwords: %s' % stopwords\n\ndef tokenize(string):\n    \"\"\" An implementation of input string tokenization that excludes stopwords\n    Args:\n        string (str): input string\n    Returns:\n        list: a list of tokens without stopwords\n    \"\"\"\n    return [x for x in simpleTokenize(string) if x not in stopwords]\n\nprint tokenize(quickbrownfox) # Should give ['quick', 'brown', ... ]","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">These are the stopwords: set([u&apos;all&apos;, u&apos;just&apos;, u&apos;being&apos;, u&apos;over&apos;, u&apos;both&apos;, u&apos;through&apos;, u&apos;yourselves&apos;, u&apos;its&apos;, u&apos;before&apos;, u&apos;with&apos;, u&apos;had&apos;, u&apos;should&apos;, u&apos;to&apos;, u&apos;only&apos;, u&apos;under&apos;, u&apos;ours&apos;, u&apos;has&apos;, u&apos;do&apos;, u&apos;them&apos;, u&apos;his&apos;, u&apos;very&apos;, u&apos;they&apos;, u&apos;not&apos;, u&apos;during&apos;, u&apos;now&apos;, u&apos;him&apos;, u&apos;nor&apos;, u&apos;did&apos;, u&apos;these&apos;, u&apos;t&apos;, u&apos;each&apos;, u&apos;where&apos;, u&apos;because&apos;, u&apos;doing&apos;, u&apos;theirs&apos;, u&apos;some&apos;, u&apos;are&apos;, u&apos;our&apos;, u&apos;ourselves&apos;, u&apos;out&apos;, u&apos;what&apos;, u&apos;for&apos;, u&apos;below&apos;, u&apos;does&apos;, u&apos;above&apos;, u&apos;between&apos;, u&apos;she&apos;, u&apos;be&apos;, u&apos;we&apos;, u&apos;after&apos;, u&apos;here&apos;, u&apos;hers&apos;, u&apos;by&apos;, u&apos;on&apos;, u&apos;about&apos;, u&apos;of&apos;, u&apos;against&apos;, u&apos;s&apos;, u&apos;or&apos;, u&apos;own&apos;, u&apos;into&apos;, u&apos;yourself&apos;, u&apos;down&apos;, u&apos;your&apos;, u&apos;from&apos;, u&apos;her&apos;, u&apos;whom&apos;, u&apos;there&apos;, u&apos;been&apos;, u&apos;few&apos;, u&apos;too&apos;, u&apos;themselves&apos;, u&apos;was&apos;, u&apos;until&apos;, u&apos;more&apos;, u&apos;himself&apos;, u&apos;that&apos;, u&apos;but&apos;, u&apos;off&apos;, u&apos;herself&apos;, u&apos;than&apos;, u&apos;those&apos;, u&apos;he&apos;, u&apos;me&apos;, u&apos;myself&apos;, u&apos;this&apos;, u&apos;up&apos;, u&apos;will&apos;, u&apos;while&apos;, u&apos;can&apos;, u&apos;were&apos;, u&apos;my&apos;, u&apos;and&apos;, u&apos;then&apos;, u&apos;is&apos;, u&apos;in&apos;, u&apos;am&apos;, u&apos;it&apos;, u&apos;an&apos;, u&apos;as&apos;, u&apos;itself&apos;, u&apos;at&apos;, u&apos;have&apos;, u&apos;further&apos;, u&apos;their&apos;, u&apos;if&apos;, u&apos;again&apos;, u&apos;no&apos;, u&apos;when&apos;, u&apos;same&apos;, u&apos;any&apos;, u&apos;how&apos;, u&apos;other&apos;, u&apos;which&apos;, u&apos;you&apos;, u&apos;who&apos;, u&apos;most&apos;, u&apos;such&apos;, u&apos;why&apos;, u&apos;a&apos;, u&apos;don&apos;, u&apos;i&apos;, u&apos;having&apos;, u&apos;so&apos;, u&apos;the&apos;, u&apos;yours&apos;, u&apos;once&apos;])\n[&apos;quick&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;jumps&apos;, &apos;lazy&apos;, &apos;dog&apos;]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482482184114E12,"submitTime":1.482482177834E12,"finishTime":1.482482184287E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"657f18a5-7778-46a3-b759-4a3164650a51"},{"version":"CommandV1","origId":2216104459508890,"guid":"f24fa5ac-29be-468f-a9f1-78a5324cbcc1","subtype":"command","commandType":"auto","position":19.0,"command":"# TEST Removing stopwords (1b)\nTest.assertEquals(tokenize(\"Why a the?\"), [], 'tokenize should remove all stopwords')\nTest.assertEquals(tokenize(\"Being at the_?\"), ['the_'], 'tokenize should handle non-stopwords')\nTest.assertEquals(tokenize(quickbrownfox), ['quick','brown','fox','jumps','lazy','dog'],\n                    'tokenize should handle sample text')","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">1 test passed.\n1 test passed.\n1 test passed.\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482482184292E12,"submitTime":1.482482177849E12,"finishTime":1.482482184332E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"644dea04-9303-49ee-a7e6-1e60cde94259"},{"version":"CommandV1","origId":2216104459508891,"guid":"a46ea6e3-13af-4c18-a822-a62f278da479","subtype":"command","commandType":"auto","position":20.0,"command":"%md\n#### **(1c) Tokenizing the small datasets**\nNow let's tokenize the two *small* datasets. For each ID in a dataset, `tokenize` the values, and then count the total number of tokens.\nHow many tokens, total, are there in the two datasets?","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482177864E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4e50f078-59fa-4c34-942f-2d8ce2ee570b"},{"version":"CommandV1","origId":2216104459508892,"guid":"4aeb927c-2a5b-4af5-b4f5-b67b29f2431f","subtype":"command","commandType":"auto","position":21.0,"command":"# TODO: Replace <FILL IN> with appropriate code\namazonRecToToken = amazonSmall.map(lambda x: (x[0], tokenize(x[1])))\ngoogleRecToToken = googleSmall.map(lambda x: (x[0], tokenize(x[1])))\n\ndef countTokens(vendorRDD):\n    \"\"\" Count and return the number of tokens\n    Args:\n        vendorRDD (RDD of (recordId, tokenizedValue)): Pair tuple of record ID to tokenized output\n    Returns:\n        count: count of all tokens\n    \"\"\"\n    return vendorRDD.map(lambda x: len(x[1])).sum()\n\ntotalTokens = countTokens(amazonRecToToken) + countTokens(googleRecToToken)\nprint 'There are %s tokens in the combined datasets' % totalTokens","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">There are 22520 tokens in the combined datasets\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 37.0 failed 1 times, most recent failure: Lost task 1.0 in stage 37.0 (TID 119, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-13-47202978b793&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     12</span>     <span class=\"ansigreen\">return</span> vendorRDD<span class=\"ansiyellow\">.</span>reduce<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">,</span> y<span class=\"ansiyellow\">:</span> len<span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span> <span class=\"ansiyellow\">+</span> len<span class=\"ansiyellow\">(</span>y<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     13</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 14</span><span class=\"ansiyellow\"> </span>totalTokens <span class=\"ansiyellow\">=</span> countTokens<span class=\"ansiyellow\">(</span>amazonRecToToken<span class=\"ansiyellow\">)</span> <span class=\"ansiyellow\">+</span> countTokens<span class=\"ansiyellow\">(</span>googleRecToToken<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     15</span> <span class=\"ansigreen\">print</span> <span class=\"ansiblue\">&apos;There are %s tokens in the combined datasets&apos;</span> <span class=\"ansiyellow\">%</span> totalTokens<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">&lt;ipython-input-13-47202978b793&gt;</span> in <span class=\"ansicyan\">countTokens</span><span class=\"ansiblue\">(vendorRDD)</span>\n<span class=\"ansigreen\">     10</span>         count<span class=\"ansiyellow\">:</span> count of all tokens<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     11</span>     &quot;&quot;&quot;\n<span class=\"ansigreen\">---&gt; 12</span><span class=\"ansiyellow\">     </span><span class=\"ansigreen\">return</span> vendorRDD<span class=\"ansiyellow\">.</span>reduce<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">,</span> y<span class=\"ansiyellow\">:</span> len<span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span> <span class=\"ansiyellow\">+</span> len<span class=\"ansiyellow\">(</span>y<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     13</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     14</span> totalTokens <span class=\"ansiyellow\">=</span> countTokens<span class=\"ansiyellow\">(</span>amazonRecToToken<span class=\"ansiyellow\">)</span> <span class=\"ansiyellow\">+</span> countTokens<span class=\"ansiyellow\">(</span>googleRecToToken<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">reduce</span><span class=\"ansiblue\">(self, f)</span>\n<span class=\"ansigreen\">    800</span>             <span class=\"ansigreen\">yield</span> reduce<span class=\"ansiyellow\">(</span>f<span class=\"ansiyellow\">,</span> iterator<span class=\"ansiyellow\">,</span> initial<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    801</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 802</span><span class=\"ansiyellow\">         </span>vals <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>mapPartitions<span class=\"ansiyellow\">(</span>func<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    803</span>         <span class=\"ansigreen\">if</span> vals<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    804</span>             <span class=\"ansigreen\">return</span> reduce<span class=\"ansiyellow\">(</span>f<span class=\"ansiyellow\">,</span> vals<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">collect</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    774</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">    775</span>         <span class=\"ansigreen\">with</span> SCCallSiteSync<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>context<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">as</span> css<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 776</span><span class=\"ansiyellow\">             </span>port <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>ctx<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>PythonRDD<span class=\"ansiyellow\">.</span>collectAndServe<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jrdd<span class=\"ansiyellow\">.</span>rdd<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    777</span>         <span class=\"ansigreen\">return</span> list<span class=\"ansiyellow\">(</span>_load_from_socket<span class=\"ansiyellow\">(</span>port<span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>_jrdd_deserializer<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    778</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1131</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1132</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1133</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1134</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1135</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    317</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    318</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 319</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    320</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    321</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 37.0 failed 1 times, most recent failure: Lost task 1.0 in stage 37.0 (TID 119, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 172, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 800, in func\n    yield reduce(f, iterator, initial)\n  File &quot;&lt;ipython-input-13-47202978b793&gt;&quot;, line 12, in &lt;lambda&gt;\nTypeError: &apos;int&apos; object has no attribute &apos;__getitem__&apos;\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.&lt;init&gt;(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1891)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1904)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1917)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:911)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor103.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 172, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 800, in func\n    yield reduce(f, iterator, initial)\n  File &quot;&lt;ipython-input-13-47202978b793&gt;&quot;, line 12, in &lt;lambda&gt;\nTypeError: &apos;int&apos; object has no attribute &apos;__getitem__&apos;\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.&lt;init&gt;(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n\n</div>","workflows":[],"startTime":1.482482184337E12,"submitTime":1.482482177885E12,"finishTime":1.482482184566E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d4b7ab46-711d-4bf3-bde9-3045294e29cf"},{"version":"CommandV1","origId":2216104459508893,"guid":"3ac0a1c9-fc55-4157-a39e-096d6690c4c1","subtype":"command","commandType":"auto","position":22.0,"command":"# TEST Tokenizing the small datasets (1c)\nTest.assertEquals(totalTokens, 22520, 'incorrect totalTokens')","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">1 test passed.\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482482184572E12,"submitTime":1.482482177901E12,"finishTime":1.48248218461E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"5957e897-db13-4b33-85e7-a28a0bcab047"},{"version":"CommandV1","origId":2216104459508894,"guid":"7d74116a-281b-4009-b68c-73031a9490f6","subtype":"command","commandType":"auto","position":23.0,"command":"%md\n#### **(1d) Amazon record with the most tokens**\nWhich Amazon record has the biggest number of tokens?\nIn other words, you want to sort the records and get the one with the largest count of tokens.","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482177916E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c1bdbb3b-96d9-47c9-ab5c-849ce2620bf0"},{"version":"CommandV1","origId":2216104459508895,"guid":"e6d6357a-d854-4f95-80fb-2006b85e262e","subtype":"command","commandType":"auto","position":24.0,"command":"# TODO: Replace <FILL IN> with appropriate code\ndef findBiggestRecord(vendorRDD):\n    \"\"\" Find and return the record with the largest number of tokens\n    Args:\n        vendorRDD (RDD of (recordId, tokens)): input Pair Tuple of record ID and tokens\n    Returns:\n        list: a list of 1 Pair Tuple of record ID and tokens\n    \"\"\"\n    return vendorRDD.takeOrdered(1, lambda x: -len(x[1]))\n\nbiggestRecordAmazon = findBiggestRecord(amazonRecToToken)\nprint 'The Amazon record with ID \"%s\" has the most tokens (%s)' % (biggestRecordAmazon[0][0],\n                                                                   len(biggestRecordAmazon[0][1]))","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">The Amazon record with ID &quot;b000o24l3q&quot; has the most tokens (1547)\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 73.0 failed 1 times, most recent failure: Lost task 0.0 in stage 73.0 (TID 226, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-55-491a7c489bcf&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      9</span>     <span class=\"ansigreen\">return</span> vendorRDD<span class=\"ansiyellow\">.</span>takeOrdered<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">,</span> <span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">-</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     10</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 11</span><span class=\"ansiyellow\"> </span>biggestRecordAmazon <span class=\"ansiyellow\">=</span> findBiggestRecord<span class=\"ansiyellow\">(</span>amazonRecToToken<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     12</span> print &apos;The Amazon record with ID &quot;%s&quot; has the most tokens (%s)&apos; % (biggestRecordAmazon[0][0],\n<span class=\"ansigreen\">     13</span>                                                                    len(biggestRecordAmazon[0][1]))\n\n<span class=\"ansigreen\">&lt;ipython-input-55-491a7c489bcf&gt;</span> in <span class=\"ansicyan\">findBiggestRecord</span><span class=\"ansiblue\">(vendorRDD)</span>\n<span class=\"ansigreen\">      7</span>         list<span class=\"ansiyellow\">:</span> a list of <span class=\"ansicyan\">1</span> Pair Tuple of record ID <span class=\"ansigreen\">and</span> tokens<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      8</span>     &quot;&quot;&quot;\n<span class=\"ansigreen\">----&gt; 9</span><span class=\"ansiyellow\">     </span><span class=\"ansigreen\">return</span> vendorRDD<span class=\"ansiyellow\">.</span>takeOrdered<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">,</span> <span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">-</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     10</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     11</span> biggestRecordAmazon <span class=\"ansiyellow\">=</span> findBiggestRecord<span class=\"ansiyellow\">(</span>amazonRecToToken<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">takeOrdered</span><span class=\"ansiblue\">(self, num, key)</span>\n<span class=\"ansigreen\">   1255</span>             <span class=\"ansigreen\">return</span> heapq<span class=\"ansiyellow\">.</span>nsmallest<span class=\"ansiyellow\">(</span>num<span class=\"ansiyellow\">,</span> a <span class=\"ansiyellow\">+</span> b<span class=\"ansiyellow\">,</span> key<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1256</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 1257</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>mapPartitions<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> it<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">[</span>heapq<span class=\"ansiyellow\">.</span>nsmallest<span class=\"ansiyellow\">(</span>num<span class=\"ansiyellow\">,</span> it<span class=\"ansiyellow\">,</span> key<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>reduce<span class=\"ansiyellow\">(</span>merge<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1258</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1259</span>     <span class=\"ansigreen\">def</span> take<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> num<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">reduce</span><span class=\"ansiblue\">(self, f)</span>\n<span class=\"ansigreen\">    800</span>             <span class=\"ansigreen\">yield</span> reduce<span class=\"ansiyellow\">(</span>f<span class=\"ansiyellow\">,</span> iterator<span class=\"ansiyellow\">,</span> initial<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    801</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 802</span><span class=\"ansiyellow\">         </span>vals <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>mapPartitions<span class=\"ansiyellow\">(</span>func<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    803</span>         <span class=\"ansigreen\">if</span> vals<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    804</span>             <span class=\"ansigreen\">return</span> reduce<span class=\"ansiyellow\">(</span>f<span class=\"ansiyellow\">,</span> vals<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">collect</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    774</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">    775</span>         <span class=\"ansigreen\">with</span> SCCallSiteSync<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>context<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">as</span> css<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 776</span><span class=\"ansiyellow\">             </span>port <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>ctx<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>PythonRDD<span class=\"ansiyellow\">.</span>collectAndServe<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jrdd<span class=\"ansiyellow\">.</span>rdd<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    777</span>         <span class=\"ansigreen\">return</span> list<span class=\"ansiyellow\">(</span>_load_from_socket<span class=\"ansiyellow\">(</span>port<span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>_jrdd_deserializer<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    778</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1131</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1132</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1133</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1134</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1135</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    317</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    318</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 319</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    320</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    321</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 73.0 failed 1 times, most recent failure: Lost task 0.0 in stage 73.0 (TID 226, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 172, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 317, in func\n    return f(iterator)\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 1257, in &lt;lambda&gt;\n    return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)\n  File &quot;/usr/lib/python2.7/heapq.py&quot;, line 412, in nsmallest\n    return [min(chain(head, it), key=key)]\n  File &quot;&lt;ipython-input-55-491a7c489bcf&gt;&quot;, line 9, in &lt;lambda&gt;\nTypeError: bad operand type for unary -: &apos;list&apos;\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.&lt;init&gt;(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1891)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1904)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1917)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:911)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor103.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 172, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 317, in func\n    return f(iterator)\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 1257, in &lt;lambda&gt;\n    return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)\n  File &quot;/usr/lib/python2.7/heapq.py&quot;, line 412, in nsmallest\n    return [min(chain(head, it), key=key)]\n  File &quot;&lt;ipython-input-55-491a7c489bcf&gt;&quot;, line 9, in &lt;lambda&gt;\nTypeError: bad operand type for unary -: &apos;list&apos;\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.&lt;init&gt;(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n\n</div>","workflows":[],"startTime":1.482482184615E12,"submitTime":1.482482177936E12,"finishTime":1.482482184737E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"559b8372-910a-44c9-8a7c-b5b2b3fcf711"},{"version":"CommandV1","origId":2216104459508896,"guid":"429fbc8f-df05-4c06-bf33-06d7b549c6e3","subtype":"command","commandType":"auto","position":25.0,"command":"# TEST Amazon record with the most tokens (1d)\nTest.assertEquals(biggestRecordAmazon[0][0], 'b000o24l3q', 'incorrect biggestRecordAmazon')\nTest.assertEquals(len(biggestRecordAmazon[0][1]), 1547, 'incorrect len for biggestRecordAmazon')","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">1 test passed.\n1 test passed.\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482482184742E12,"submitTime":1.482482177952E12,"finishTime":1.482482184794E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f6f66501-0f7a-43dc-86cf-c3e5267b247a"},{"version":"CommandV1","origId":2216104459508897,"guid":"c524c447-6433-449d-8fcd-cc3a4a6e0186","subtype":"command","commandType":"auto","position":26.0,"command":"%md\n#### **Part 2: ER as Text Similarity - Weighted Bag-of-Words using TF-IDF**\nBag-of-words comparisons are not very good when all tokens are treated the same: some tokens are more important than others. Weights give us a way to specify which tokens to favor. With weights, when we compare documents, instead of counting common tokens, we sum up the weights of common tokens. A good heuristic for assigning weights is called \"Term-Frequency/Inverse-Document-Frequency,\" or [TF-IDF][tfidf] for short.\n \n**TF**\n \nTF rewards tokens that appear many times in the same document. It is computed as the frequency of a token in a document, that is, if document *d* contains 100 tokens and token *t* appears in *d* 5 times, then the TF weight of *t* in *d* is *5/100 = 1/20*. The intuition for TF is that if a word occurs often in a document, then it is more important to the meaning of the document.\n \n**IDF**\n \nIDF rewards tokens that are rare overall in a dataset. The intuition is that it is more significant if two documents share a rare word than a common one. IDF weight for a token, *t*, in a set of documents, *U*, is computed as follows:\n* Let *N* be the total number of documents in *U*\n* Find *n(t)*, the number of documents in *U* that contain *t*\n* Then *IDF(t) = N/n(t)*.\n \nNote that *n(t)/N* is the frequency of *t* in *U*, and *N/n(t)* is the inverse frequency.\n \n> **Note on terminology**: Sometimes token weights depend on the document the token belongs to, that is, the same token may have a different weight when it's found in different documents.  We call these weights *local* weights.  TF is an example of a local weight, because it depends on the length of the source.  On the other hand, some token weights only depend on the token, and are the same everywhere that token is found.  We call these weights *global*, and IDF is one such weight.\n \n**TF-IDF**\n \nFinally, to bring it all together, the total TF-IDF weight for a token in a document is the product of its TF and IDF weights.\n[tfidf]: https://en.wikipedia.org/wiki/Tf%E2%80%93idf","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482177968E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b5448b80-0a2f-47fe-8dac-b6135aebe2cb"},{"version":"CommandV1","origId":2216104459508898,"guid":"c53b71ef-d44f-4dc5-8f9a-51e07c759439","subtype":"command","commandType":"auto","position":27.0,"command":"%md\n#### **(2a) Implement a TF function**\n \nImplement `tf(tokens)` that takes a list of tokens and returns a Python [dictionary](https://docs.python.org/2/tutorial/datastructures.html#dictionaries) mapping tokens to TF weights.\n \nThe steps your function should perform are:\n* Create an empty Python dictionary\n* For each of the tokens in the input `tokens` list, count 1 for each occurance and add the token to the dictionary\n* For each of the tokens in the dictionary, divide the token's count by the total number of tokens in the input `tokens` list","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482177988E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"31b9941e-3bc6-4880-aff7-b54362c46f97"},{"version":"CommandV1","origId":2216104459508977,"guid":"81ba11cc-348d-49ef-b317-e99c900e54bf","subtype":"command","commandType":"auto","position":27.5,"command":"tokens = tokenize(quickbrownfox)\ntks = sc.parallelize(tokens)\n\ntks.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x+y).map(lambda x: x[1]*1./len(tokens)).collect()\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">51</span><span class=\"ansired\">]: </span>\n[0.16666666666666666,\n 0.16666666666666666,\n 0.16666666666666666,\n 0.16666666666666666,\n 0.16666666666666666,\n 0.16666666666666666]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482482184799E12,"submitTime":1.482482178008E12,"finishTime":1.482482185025E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"316a720f-2d51-4b02-ba5f-9f258f22d0ed"},{"version":"CommandV1","origId":2216104459508899,"guid":"f7792479-8211-4213-b144-e727e574bc33","subtype":"command","commandType":"auto","position":28.0,"command":"# TODO: Replace <FILL IN> with appropriate code\ndef tf(tokens):\n    \"\"\" Compute TF\n    Args:\n        tokens (list of str): input list of tokens from tokenize\n    Returns:\n        dictionary: a dictionary of tokens to its TF values\n    \"\"\"\n    tks = sc.parallelize(tokens)\n    counts = tks.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x+y)\n    return dict(counts.map(lambda x: (x[0], 1. * x[1]/len(tokens))).collect())\n\nprint tf(tokenize(quickbrownfox)) # Should give { 'quick': 0.1666 ... }\n","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">{&apos;brown&apos;: 0.16666666666666666, &apos;lazy&apos;: 0.16666666666666666, &apos;jumps&apos;: 0.16666666666666666, &apos;fox&apos;: 0.16666666666666666, &apos;dog&apos;: 0.16666666666666666, &apos;quick&apos;: 0.16666666666666666}\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<span class=\"ansired\">Exception</span>: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Exception</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-58-6c8c2292d7ac&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     12</span>     <span class=\"ansigreen\">return</span> counts<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">/</span>counts<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     13</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 14</span><span class=\"ansiyellow\"> </span><span class=\"ansigreen\">print</span> tf<span class=\"ansiyellow\">(</span>tokenize<span class=\"ansiyellow\">(</span>quickbrownfox<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Should give { &apos;quick&apos;: 0.1666 ... }</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">__repr__</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    199</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    200</span>     <span class=\"ansigreen\">def</span> __repr__<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 201</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_jrdd<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    202</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    203</span>     <span class=\"ansigreen\">def</span> __getnewargs__<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">_jrdd</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">   2401</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2402</span>         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n<span class=\"ansigreen\">-&gt; 2403</span><span class=\"ansiyellow\">                                       self._jrdd_deserializer, profiler)\n</span><span class=\"ansigreen\">   2404</span>         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n<span class=\"ansigreen\">   2405</span>                                              self.preservesPartitioning)\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">_wrap_function</span><span class=\"ansiblue\">(sc, func, deserializer, serializer, profiler)</span>\n<span class=\"ansigreen\">   2334</span>     <span class=\"ansigreen\">assert</span> serializer<span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;serializer should not be empty&quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2335</span>     command <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">(</span>func<span class=\"ansiyellow\">,</span> profiler<span class=\"ansiyellow\">,</span> deserializer<span class=\"ansiyellow\">,</span> serializer<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 2336</span><span class=\"ansiyellow\">     </span>pickled_command<span class=\"ansiyellow\">,</span> broadcast_vars<span class=\"ansiyellow\">,</span> env<span class=\"ansiyellow\">,</span> includes <span class=\"ansiyellow\">=</span> _prepare_for_python_RDD<span class=\"ansiyellow\">(</span>sc<span class=\"ansiyellow\">,</span> command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2337</span>     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n<span class=\"ansigreen\">   2338</span>                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">_prepare_for_python_RDD</span><span class=\"ansiblue\">(sc, command)</span>\n<span class=\"ansigreen\">   2313</span>     <span class=\"ansired\"># the serialized command will be compressed by broadcast</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2314</span>     ser <span class=\"ansiyellow\">=</span> CloudPickleSerializer<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 2315</span><span class=\"ansiyellow\">     </span>pickled_command <span class=\"ansiyellow\">=</span> ser<span class=\"ansiyellow\">.</span>dumps<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2316</span>     <span class=\"ansigreen\">if</span> len<span class=\"ansiyellow\">(</span>pickled_command<span class=\"ansiyellow\">)</span> <span class=\"ansiyellow\">&gt;</span> <span class=\"ansiyellow\">(</span><span class=\"ansicyan\">1</span> <span class=\"ansiyellow\">&lt;&lt;</span> <span class=\"ansicyan\">20</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span>  <span class=\"ansired\"># 1M</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2317</span>         <span class=\"ansired\"># The broadcast will have same life cycle as created PythonRDD</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/serializers.py</span> in <span class=\"ansicyan\">dumps</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    426</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    427</span>     <span class=\"ansigreen\">def</span> dumps<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 428</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> cloudpickle<span class=\"ansiyellow\">.</span>dumps<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">2</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    429</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    430</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">dumps</span><span class=\"ansiblue\">(obj, protocol)</span>\n<span class=\"ansigreen\">    655</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    656</span>     cp <span class=\"ansiyellow\">=</span> CloudPickler<span class=\"ansiyellow\">(</span>file<span class=\"ansiyellow\">,</span>protocol<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 657</span><span class=\"ansiyellow\">     </span>cp<span class=\"ansiyellow\">.</span>dump<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    658</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    659</span>     <span class=\"ansigreen\">return</span> file<span class=\"ansiyellow\">.</span>getvalue<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">dump</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    105</span>         self<span class=\"ansiyellow\">.</span>inject_addons<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    106</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 107</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> Pickler<span class=\"ansiyellow\">.</span>dump<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    108</span>         <span class=\"ansigreen\">except</span> RuntimeError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    109</span>             <span class=\"ansigreen\">if</span> <span class=\"ansiblue\">&apos;recursion&apos;</span> <span class=\"ansigreen\">in</span> e<span class=\"ansiyellow\">.</span>args<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">dump</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    222</span>         <span class=\"ansigreen\">if</span> self<span class=\"ansiyellow\">.</span>proto <span class=\"ansiyellow\">&gt;=</span> <span class=\"ansicyan\">2</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    223</span>             self<span class=\"ansiyellow\">.</span>write<span class=\"ansiyellow\">(</span>PROTO <span class=\"ansiyellow\">+</span> chr<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>proto<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 224</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>save<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    225</span>         self<span class=\"ansiyellow\">.</span>write<span class=\"ansiyellow\">(</span>STOP<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    226</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save_tuple</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    566</span>         write<span class=\"ansiyellow\">(</span>MARK<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    567</span>         <span class=\"ansigreen\">for</span> element <span class=\"ansigreen\">in</span> obj<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 568</span><span class=\"ansiyellow\">             </span>save<span class=\"ansiyellow\">(</span>element<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    569</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    570</span>         <span class=\"ansigreen\">if</span> id<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">in</span> memo<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function</span><span class=\"ansiblue\">(self, obj, name)</span>\n<span class=\"ansigreen\">    202</span>             klass <span class=\"ansiyellow\">=</span> getattr<span class=\"ansiyellow\">(</span>themodule<span class=\"ansiyellow\">,</span> name<span class=\"ansiyellow\">,</span> None<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    203</span>             <span class=\"ansigreen\">if</span> klass <span class=\"ansigreen\">is</span> None <span class=\"ansigreen\">or</span> klass <span class=\"ansigreen\">is</span> <span class=\"ansigreen\">not</span> obj<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 204</span><span class=\"ansiyellow\">                 </span>self<span class=\"ansiyellow\">.</span>save_function_tuple<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    205</span>                 <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    206</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function_tuple</span><span class=\"ansiblue\">(self, func)</span>\n<span class=\"ansigreen\">    239</span>         <span class=\"ansired\"># create a skeleton function object and memoize it</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    240</span>         save<span class=\"ansiyellow\">(</span>_make_skel_func<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 241</span><span class=\"ansiyellow\">         </span>save<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">(</span>code<span class=\"ansiyellow\">,</span> closure<span class=\"ansiyellow\">,</span> base_globals<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    242</span>         write<span class=\"ansiyellow\">(</span>pickle<span class=\"ansiyellow\">.</span>REDUCE<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    243</span>         self<span class=\"ansiyellow\">.</span>memoize<span class=\"ansiyellow\">(</span>func<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save_tuple</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    552</span>         <span class=\"ansigreen\">if</span> n <span class=\"ansiyellow\">&lt;=</span> <span class=\"ansicyan\">3</span> <span class=\"ansigreen\">and</span> proto <span class=\"ansiyellow\">&gt;=</span> <span class=\"ansicyan\">2</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    553</span>             <span class=\"ansigreen\">for</span> element <span class=\"ansigreen\">in</span> obj<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 554</span><span class=\"ansiyellow\">                 </span>save<span class=\"ansiyellow\">(</span>element<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    555</span>             <span class=\"ansired\"># Subtle.  Same as in the big comment below.</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    556</span>             <span class=\"ansigreen\">if</span> id<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">in</span> memo<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save_list</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    604</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    605</span>         self<span class=\"ansiyellow\">.</span>memoize<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 606</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>_batch_appends<span class=\"ansiyellow\">(</span>iter<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    607</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    608</span>     dispatch<span class=\"ansiyellow\">[</span>ListType<span class=\"ansiyellow\">]</span> <span class=\"ansiyellow\">=</span> save_list<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">_batch_appends</span><span class=\"ansiblue\">(self, items)</span>\n<span class=\"ansigreen\">    637</span>                 write<span class=\"ansiyellow\">(</span>MARK<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    638</span>                 <span class=\"ansigreen\">for</span> x <span class=\"ansigreen\">in</span> tmp<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 639</span><span class=\"ansiyellow\">                     </span>save<span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    640</span>                 write<span class=\"ansiyellow\">(</span>APPENDS<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    641</span>             <span class=\"ansigreen\">elif</span> n<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function</span><span class=\"ansiblue\">(self, obj, name)</span>\n<span class=\"ansigreen\">    202</span>             klass <span class=\"ansiyellow\">=</span> getattr<span class=\"ansiyellow\">(</span>themodule<span class=\"ansiyellow\">,</span> name<span class=\"ansiyellow\">,</span> None<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    203</span>             <span class=\"ansigreen\">if</span> klass <span class=\"ansigreen\">is</span> None <span class=\"ansigreen\">or</span> klass <span class=\"ansigreen\">is</span> <span class=\"ansigreen\">not</span> obj<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 204</span><span class=\"ansiyellow\">                 </span>self<span class=\"ansiyellow\">.</span>save_function_tuple<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    205</span>                 <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    206</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function_tuple</span><span class=\"ansiblue\">(self, func)</span>\n<span class=\"ansigreen\">    239</span>         <span class=\"ansired\"># create a skeleton function object and memoize it</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    240</span>         save<span class=\"ansiyellow\">(</span>_make_skel_func<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 241</span><span class=\"ansiyellow\">         </span>save<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">(</span>code<span class=\"ansiyellow\">,</span> closure<span class=\"ansiyellow\">,</span> base_globals<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    242</span>         write<span class=\"ansiyellow\">(</span>pickle<span class=\"ansiyellow\">.</span>REDUCE<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    243</span>         self<span class=\"ansiyellow\">.</span>memoize<span class=\"ansiyellow\">(</span>func<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save_tuple</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    552</span>         <span class=\"ansigreen\">if</span> n <span class=\"ansiyellow\">&lt;=</span> <span class=\"ansicyan\">3</span> <span class=\"ansigreen\">and</span> proto <span class=\"ansiyellow\">&gt;=</span> <span class=\"ansicyan\">2</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    553</span>             <span class=\"ansigreen\">for</span> element <span class=\"ansigreen\">in</span> obj<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 554</span><span class=\"ansiyellow\">                 </span>save<span class=\"ansiyellow\">(</span>element<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    555</span>             <span class=\"ansired\"># Subtle.  Same as in the big comment below.</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    556</span>             <span class=\"ansigreen\">if</span> id<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">in</span> memo<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save_list</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    604</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    605</span>         self<span class=\"ansiyellow\">.</span>memoize<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 606</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>_batch_appends<span class=\"ansiyellow\">(</span>iter<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    607</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    608</span>     dispatch<span class=\"ansiyellow\">[</span>ListType<span class=\"ansiyellow\">]</span> <span class=\"ansiyellow\">=</span> save_list<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">_batch_appends</span><span class=\"ansiblue\">(self, items)</span>\n<span class=\"ansigreen\">    640</span>                 write<span class=\"ansiyellow\">(</span>APPENDS<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    641</span>             <span class=\"ansigreen\">elif</span> n<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 642</span><span class=\"ansiyellow\">                 </span>save<span class=\"ansiyellow\">(</span>tmp<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    643</span>                 write<span class=\"ansiyellow\">(</span>APPEND<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    644</span>             <span class=\"ansired\"># else tmp is empty, and we&apos;re done</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function</span><span class=\"ansiblue\">(self, obj, name)</span>\n<span class=\"ansigreen\">    196</span>         <span class=\"ansigreen\">if</span> islambda<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">or</span> obj<span class=\"ansiyellow\">.</span>__code__<span class=\"ansiyellow\">.</span>co_filename <span class=\"ansiyellow\">==</span> <span class=\"ansiblue\">&apos;&lt;stdin&gt;&apos;</span> <span class=\"ansigreen\">or</span> themodule <span class=\"ansigreen\">is</span> None<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    197</span>             <span class=\"ansired\">#print(&quot;save global&quot;, islambda(obj), obj.__code__.co_filename, modname, themodule)</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 198</span><span class=\"ansiyellow\">             </span>self<span class=\"ansiyellow\">.</span>save_function_tuple<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    199</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    200</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function_tuple</span><span class=\"ansiblue\">(self, func)</span>\n<span class=\"ansigreen\">    239</span>         <span class=\"ansired\"># create a skeleton function object and memoize it</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    240</span>         save<span class=\"ansiyellow\">(</span>_make_skel_func<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 241</span><span class=\"ansiyellow\">         </span>save<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">(</span>code<span class=\"ansiyellow\">,</span> closure<span class=\"ansiyellow\">,</span> base_globals<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    242</span>         write<span class=\"ansiyellow\">(</span>pickle<span class=\"ansiyellow\">.</span>REDUCE<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    243</span>         self<span class=\"ansiyellow\">.</span>memoize<span class=\"ansiyellow\">(</span>func<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save_tuple</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    552</span>         <span class=\"ansigreen\">if</span> n <span class=\"ansiyellow\">&lt;=</span> <span class=\"ansicyan\">3</span> <span class=\"ansigreen\">and</span> proto <span class=\"ansiyellow\">&gt;=</span> <span class=\"ansicyan\">2</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    553</span>             <span class=\"ansigreen\">for</span> element <span class=\"ansigreen\">in</span> obj<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 554</span><span class=\"ansiyellow\">                 </span>save<span class=\"ansiyellow\">(</span>element<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    555</span>             <span class=\"ansired\"># Subtle.  Same as in the big comment below.</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    556</span>             <span class=\"ansigreen\">if</span> id<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">in</span> memo<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save_list</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    604</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    605</span>         self<span class=\"ansiyellow\">.</span>memoize<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 606</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>_batch_appends<span class=\"ansiyellow\">(</span>iter<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    607</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    608</span>     dispatch<span class=\"ansiyellow\">[</span>ListType<span class=\"ansiyellow\">]</span> <span class=\"ansiyellow\">=</span> save_list<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">_batch_appends</span><span class=\"ansiblue\">(self, items)</span>\n<span class=\"ansigreen\">    640</span>                 write<span class=\"ansiyellow\">(</span>APPENDS<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    641</span>             <span class=\"ansigreen\">elif</span> n<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 642</span><span class=\"ansiyellow\">                 </span>save<span class=\"ansiyellow\">(</span>tmp<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    643</span>                 write<span class=\"ansiyellow\">(</span>APPEND<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    644</span>             <span class=\"ansired\"># else tmp is empty, and we&apos;re done</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    304</span>             reduce <span class=\"ansiyellow\">=</span> getattr<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;__reduce_ex__&quot;</span><span class=\"ansiyellow\">,</span> None<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    305</span>             <span class=\"ansigreen\">if</span> reduce<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 306</span><span class=\"ansiyellow\">                 </span>rv <span class=\"ansiyellow\">=</span> reduce<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>proto<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    307</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    308</span>                 reduce <span class=\"ansiyellow\">=</span> getattr<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;__reduce__&quot;</span><span class=\"ansiyellow\">,</span> None<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">__getnewargs__</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    204</span>         <span class=\"ansired\"># This method is called when attempting to pickle an RDD, which is always an error:</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    205</span>         raise Exception(\n<span class=\"ansigreen\">--&gt; 206</span><span class=\"ansiyellow\">             </span><span class=\"ansiblue\">&quot;It appears that you are attempting to broadcast an RDD or reference an RDD from an &quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    207</span>             <span class=\"ansiblue\">&quot;action or transformation. RDD transformations and actions can only be invoked by the &quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    208</span>             <span class=\"ansiblue\">&quot;driver, not inside of other transformations; for example, &quot;</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">Exception</span>: It appears that you are attempting to broadcast an RDD or reference an RDD from an action or transformation. RDD transformations and actions can only be invoked by the driver, not inside of other transformations; for example, rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the rdd1.map transformation. For more information, see SPARK-5063.\n</div>","workflows":[],"startTime":1.482482185031E12,"submitTime":1.482482178023E12,"finishTime":1.482482185262E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ab9fe85b-911e-40dc-914c-12ee3f2bc90d"},{"version":"CommandV1","origId":2216104459508900,"guid":"f0803699-54d4-4e8b-9033-161c88a2ffb6","subtype":"command","commandType":"auto","position":29.0,"command":"# TEST Implement a TF function (2a)\ntf_test = tf(tokenize(quickbrownfox))\nTest.assertEquals(tf_test, {'brown': 0.16666666666666666, 'lazy': 0.16666666666666666,\n                             'jumps': 0.16666666666666666, 'fox': 0.16666666666666666,\n                             'dog': 0.16666666666666666, 'quick': 0.16666666666666666},\n                    'incorrect result for tf on sample text')\ntf_test2 = tf(tokenize('one_ one_ two!'))\nTest.assertEquals(tf_test2, {'one_': 0.6666666666666666, 'two': 0.3333333333333333},\n                    'incorrect result for tf test')","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">1 test passed.\n1 test passed.\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482482185269E12,"submitTime":1.482482178041E12,"finishTime":1.482482185697E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"73a6d98a-5e53-4980-991c-bca1c19dca7d"},{"version":"CommandV1","origId":2216104459508901,"guid":"edbf025d-7cea-4adb-a110-cbfc87665335","subtype":"command","commandType":"auto","position":30.0,"command":"%md\n#### **(2b) Create a corpus**\nCreate a pair RDD called `corpusRDD`, consisting of a combination of the two small datasets, `amazonRecToToken` and `googleRecToToken`. Each element of the `corpusRDD` should be a pair consisting of a key from one of the small datasets (ID or URL) and the value is the associated value for that key from the small datasets.","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482178055E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b1585939-858e-4bfd-9400-71eaa77579c8"},{"version":"CommandV1","origId":2216104459508902,"guid":"7f8ed0bf-1365-4a28-84a0-17c7f54d0b59","subtype":"command","commandType":"auto","position":31.0,"command":"# TODO: Replace <FILL IN> with appropriate code\ncorpusRDD = amazonRecToToken.union(googleRecToToken)","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482482185701E12,"submitTime":1.482482178076E12,"finishTime":1.482482185742E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"153ab1b2-811e-4ba1-be86-96985017b31f"},{"version":"CommandV1","origId":2216104459508903,"guid":"1bb8553b-65ef-480c-b551-e2b8c18464a9","subtype":"command","commandType":"auto","position":32.0,"command":"# TEST Create a corpus (2b)\nTest.assertEquals(corpusRDD.count(), 400, 'incorrect corpusRDD.count()')","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">1 test passed.\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482482185749E12,"submitTime":1.48248217809E12,"finishTime":1.482482185974E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b92a91bd-ee2d-45cf-b18a-d6f7ee706d08"},{"version":"CommandV1","origId":2216104459508904,"guid":"096861c0-b165-48c1-a268-3a23de4b6c53","subtype":"command","commandType":"auto","position":33.0,"command":"%md\n#### **(2c) Implement an IDFs function**\nImplement `idfs` that assigns an IDF weight to every unique token in an RDD called `corpus`. The function should return an pair RDD where the `key` is the unique token and value is the IDF weight for the token.\n \nRecall that the IDF weight for a token, *t*, in a set of documents, *U*, is computed as follows:\n* Let *N* be the total number of documents in *U*.\n* Find *n(t)*, the number of documents in *U* that contain *t*.\n* Then *IDF(t) = N/n(t)*.\n \nThe steps your function should perform are:\n* Calculate *N*. Think about how you can calculate *N* from the input RDD.\n* Create an RDD (*not a pair RDD*) containing the unique tokens from each document in the input `corpus`. For each document, you should only include a token once, *even if it appears multiple times in that document.*\n* For each of the unique tokens, count how many times it appears in the document and then compute the IDF for that token: *N/n(t)*\n \nUse your `idfs` to compute the IDF weights for all tokens in `corpusRDD` (the combined small datasets).\nHow many unique tokens are there?","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482178104E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b7f2077a-e97b-4066-937c-94a4f97e9cdf"},{"version":"CommandV1","origId":4182197520634969,"guid":"8364320e-4e37-47ae-80f2-1e21f255d541","subtype":"command","commandType":"auto","position":33.75,"command":"idfsSmall.collectAsMap?","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Type:        </span>instancemethod\n<span class=\"ansired\">String form: </span>&lt;bound method PipelinedRDD.collectAsMap of PythonRDD[343] at collectAsMap at &lt;ipython-input-63-716ba23f1279&gt;:15&gt;\n<span class=\"ansired\">File:        </span>/databricks/spark/python/pyspark/rdd.py\n<span class=\"ansired\">Definition:  </span>idfsSmall<span class=\"ansiyellow\">.</span>collectAsMap<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansired\">Docstring:</span>\nReturn the key-value pairs in this RDD to the master as a dictionary.\n\nNote that this method should only be used if the resulting data is expected\nto be small, as all the data is loaded into the driver&apos;s memory.\n\n&gt;&gt;&gt; m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n&gt;&gt;&gt; m[1]\n2\n&gt;&gt;&gt; m[3]\n4\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482487543543E12,"submitTime":1.482487543539E12,"finishTime":1.482487543614E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"7b519e57-cd67-499d-b2d5-d3cb81793df8"},{"version":"CommandV1","origId":2216104459508905,"guid":"9cb9ccd3-9c1a-478e-8a05-84cdd85c90a0","subtype":"command","commandType":"auto","position":34.0,"command":"# TODO: Replace <FILL IN> with appropriate code\ndef idfs(corpus):\n    \"\"\" Compute IDF\n    Args:\n        corpus (RDD): input corpus\n    Returns:\n        RDD: a RDD of (token, IDF value)\n    \"\"\"\n    N = corpus.count()\n    uniqueTokens = corpus.flatMap(lambda x: list(set(x[1])))\n    tokenCountPairTuple = uniqueTokens.map(lambda x: (x, 1))\n    tokenSumPairTuple = tokenCountPairTuple.reduceByKey(lambda a, b: a + b)\n    return tokenSumPairTuple.map(lambda x: (x[0], float(N)/x[1]))\n\n\nidfsSmall = idfs(amazonRecToToken.union(googleRecToToken))\nuniqueTokenCount = idfsSmall.count()\n\nprint 'There are %s unique tokens in the small datasets.' % uniqueTokenCount","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">There are 4772 unique tokens in the small datasets.\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 138.0 failed 1 times, most recent failure: Lost task 7.0 in stage 138.0 (TID 586, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-95-0ca248017b69&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     14</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     15</span> idfsSmall <span class=\"ansiyellow\">=</span> idfs<span class=\"ansiyellow\">(</span>amazonRecToToken<span class=\"ansiyellow\">.</span>union<span class=\"ansiyellow\">(</span>googleRecToToken<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 16</span><span class=\"ansiyellow\"> </span>uniqueTokenCount <span class=\"ansiyellow\">=</span> idfsSmall<span class=\"ansiyellow\">.</span>count<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     17</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     18</span> <span class=\"ansigreen\">print</span> <span class=\"ansiblue\">&apos;There are %s unique tokens in the small datasets.&apos;</span> <span class=\"ansiyellow\">%</span> uniqueTokenCount<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">count</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">   1006</span>         <span class=\"ansicyan\">3</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1007</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">-&gt; 1008</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>mapPartitions<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> i<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">[</span>sum<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">1</span> <span class=\"ansigreen\">for</span> _ <span class=\"ansigreen\">in</span> i<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>sum<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1009</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1010</span>     <span class=\"ansigreen\">def</span> stats<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">sum</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    997</span>         <span class=\"ansicyan\">6.0</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    998</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 999</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>mapPartitions<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">[</span>sum<span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>fold<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">,</span> operator<span class=\"ansiyellow\">.</span>add<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1000</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1001</span>     <span class=\"ansigreen\">def</span> count<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">fold</span><span class=\"ansiblue\">(self, zeroValue, op)</span>\n<span class=\"ansigreen\">    871</span>         <span class=\"ansired\"># zeroValue provided to each partition is unique from the one provided</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    872</span>         <span class=\"ansired\"># to the final reduce call</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 873</span><span class=\"ansiyellow\">         </span>vals <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>mapPartitions<span class=\"ansiyellow\">(</span>func<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    874</span>         <span class=\"ansigreen\">return</span> reduce<span class=\"ansiyellow\">(</span>op<span class=\"ansiyellow\">,</span> vals<span class=\"ansiyellow\">,</span> zeroValue<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    875</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">collect</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    774</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">    775</span>         <span class=\"ansigreen\">with</span> SCCallSiteSync<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>context<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">as</span> css<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 776</span><span class=\"ansiyellow\">             </span>port <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>ctx<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>PythonRDD<span class=\"ansiyellow\">.</span>collectAndServe<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jrdd<span class=\"ansiyellow\">.</span>rdd<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    777</span>         <span class=\"ansigreen\">return</span> list<span class=\"ansiyellow\">(</span>_load_from_socket<span class=\"ansiyellow\">(</span>port<span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>_jrdd_deserializer<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    778</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1131</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1132</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1133</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1134</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1135</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    317</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    318</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 319</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    320</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    321</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 138.0 failed 1 times, most recent failure: Lost task 7.0 in stage 138.0 (TID 586, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 172, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 317, in func\n    return f(iterator)\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 1792, in combineLocally\n    merger.mergeValues(iterator)\n  File &quot;/databricks/spark/python/pyspark/shuffle.py&quot;, line 236, in mergeValues\n    for k, v in iterator:\nValueError: too many values to unpack\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.&lt;init&gt;(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1891)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1904)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1917)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:911)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor103.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 172, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 317, in func\n    return f(iterator)\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 1792, in combineLocally\n    merger.mergeValues(iterator)\n  File &quot;/databricks/spark/python/pyspark/shuffle.py&quot;, line 236, in mergeValues\n    for k, v in iterator:\nValueError: too many values to unpack\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.&lt;init&gt;(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n\n</div>","workflows":[],"startTime":1.482482186232E12,"submitTime":1.482482178151E12,"finishTime":1.482482186671E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f614d94c-2d9e-4c83-beda-7acd366fecc9"},{"version":"CommandV1","origId":2216104459508906,"guid":"5745022e-17e9-42cd-9e68-11d51d04ff83","subtype":"command","commandType":"auto","position":35.0,"command":"# TEST Implement an IDFs function (2c)\nTest.assertEquals(uniqueTokenCount, 4772, 'incorrect uniqueTokenCount')\ntokenSmallestIdf = idfsSmall.takeOrdered(1, lambda s: s[1])[0]\nTest.assertEquals(tokenSmallestIdf[0], 'software', 'incorrect smallest IDF token')\nTest.assertTrue(abs(tokenSmallestIdf[1] - 4.25531914894) < 0.0000000001,\n                'incorrect smallest IDF value')","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">1 test passed.\n1 test passed.\n1 test passed.\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482482186677E12,"submitTime":1.482482178167E12,"finishTime":1.48248218685E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f13b13dc-1114-4af2-b854-6374b5b2d47a"},{"version":"CommandV1","origId":2216104459508907,"guid":"63f51918-3841-4417-a692-8630579588ba","subtype":"command","commandType":"auto","position":36.0,"command":"%md\n#### **(2d) Tokens with the smallest IDF**\nPrint out the 11 tokens with the smallest IDF in the combined small dataset.","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.48248217818E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"487f42e5-b6c7-4257-bcc8-0dff6df71b08"},{"version":"CommandV1","origId":2216104459508908,"guid":"61087846-e038-4338-b28d-0da44134d98e","subtype":"command","commandType":"auto","position":37.0,"command":"smallIDFTokens = idfsSmall.takeOrdered(11, lambda s: s[1])\nprint smallIDFTokens","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">[(&apos;software&apos;, 4.25531914893617), (&apos;new&apos;, 6.896551724137931), (&apos;features&apos;, 6.896551724137931), (&apos;use&apos;, 7.017543859649122), (&apos;complete&apos;, 7.2727272727272725), (&apos;easy&apos;, 7.6923076923076925), (&apos;create&apos;, 8.333333333333334), (&apos;system&apos;, 8.333333333333334), (&apos;cd&apos;, 8.333333333333334), (&apos;1&apos;, 8.51063829787234), (&apos;windows&apos;, 8.51063829787234)]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482482186855E12,"submitTime":1.4824821782E12,"finishTime":1.482482186986E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"16143997-6588-4833-9c8e-1c3c3bfbb1f5"},{"version":"CommandV1","origId":2216104459508909,"guid":"3472222b-2b39-4898-8405-fd48d03ae0b8","subtype":"command","commandType":"auto","position":38.0,"command":"%md\n#### **(2e) IDF Histogram**\nPlot a histogram of IDF values.  Be sure to use appropriate scaling and bucketing for the data.\nFirst plot the histogram using `matplotlib`","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482178213E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"1ea6bb7c-0820-43b7-b490-3b809b00d238"},{"version":"CommandV1","origId":2216104459508910,"guid":"45f39e52-ca75-4820-9b2b-2ae8f184e9ad","subtype":"command","commandType":"auto","position":39.0,"command":"import matplotlib.pyplot as plt\n\nsmall_idf_values = idfsSmall.map(lambda s: s[1]).collect()\nfig = plt.figure(figsize=(8,3))\nplt.hist(small_idf_values, 50, log=True)\ndisplay(fig) \npass","commandVersion":1,"state":"finished","results":{"type":"image","data":"data:image/png;base64,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","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482482186997E12,"submitTime":1.482482178234E12,"finishTime":1.482482189361E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"5ed43734-ff66-4f86-8096-6c3c8cd0aff3"},{"version":"CommandV1","origId":2216104459508911,"guid":"621297be-8aca-40a7-b213-82e70e168006","subtype":"command","commandType":"auto","position":40.0,"command":"from pyspark.sql import Row\n\n# Create a DataFrame and visualize using display()\nidfsToCountRow = idfsSmall.map(lambda (x, y): Row(token=x, value=y))\nidfsToCountDF = sqlContext.createDataFrame(idfsToCountRow)\ndisplay(idfsToCountDF)","commandVersion":1,"state":"finished","results":{"type":"table","data":[["aided",400.0],["precise",100.0],["duplex",400.0],["dance",400.0],["breath",200.0],["themes",133.33333333333334],["known",100.0],["verses",400.0],["battle",100.0],["9999",400.0],["targeted",400.0],["layers",100.0],["content",23.529411764705884],["xml",400.0],["paris",400.0],["volume",80.0],["environment",44.44444444444444],["german",400.0],["wants",400.0],["diskcromwindows",400.0],["antique",200.0],["nightfall",400.0],["merchant",400.0],["shoot",400.0],["infinite",400.0],["lessons",44.44444444444444],["far",133.33333333333334],["puzzles",100.0],["downsample",400.0],["elona",400.0],["telling",200.0],["em",200.0],["tri",200.0],["enjoyable",400.0],["788687400220",400.0],["customizable",66.66666666666667],["large",57.142857142857146],["hexachrome",400.0],["phase",400.0],["melodies",200.0],["fractions",200.0],["enjoy",44.44444444444444],["visuals",400.0],["madcap",400.0],["synergy",200.0],["talks",200.0],["go",80.0],["licenses",57.142857142857146],["universities",200.0],["trhough",400.0],["cell",200.0],["deleted",400.0],["templates",57.142857142857146],["sims",400.0],["certificates",400.0],["authoritative",400.0],["body",80.0],["compact",80.0],["relationships",200.0],["ambience",400.0],["zoology",400.0],["hours",33.333333333333336],["growing",200.0],["esk9",400.0],["directions",400.0],["box",66.66666666666667],["cockpit",400.0],["almost",133.33333333333334],["sharks",400.0],["institute",400.0],["current",133.33333333333334],["museums",200.0],["tv",200.0],["action",33.333333333333336],["camerata",400.0],["giftseven",400.0],["beautiful",66.66666666666667],["inbound",400.0],["extrn",400.0],["informatics",400.0],["landdesigner",400.0],["300mhz",400.0],["eg04025",400.0],["psat",133.33333333333334],["mappoint",400.0],["files",14.285714285714286],["receivable",400.0],["buttons",200.0],["browse",400.0],["connector",400.0],["process",40.0],["sample",100.0],["encode",400.0],["breakthrough",400.0],["sort",133.33333333333334],["shine",400.0],["immersion",50.0],["entsec",400.0],["equivalents",400.0],["std",200.0],["timecode",400.0],["fly",200.0],["high",16.0],["truetype",400.0],["languages",100.0],["tunnel",200.0],["past",100.0],["pr",400.0],["reviews",400.0],["instantaneously",400.0],["regions",400.0],["following",100.0],["chapter",400.0],["control",33.333333333333336],["multistore",200.0],["habitats",200.0],["invoices",200.0],["10307",400.0],["topic",400.0],["claimed",400.0],["authorities",400.0],["stationery",200.0],["scheduling",200.0],["simple",22.22222222222222],["creates",133.33333333333334],["answer",200.0],["trailers",400.0],["steve",200.0],["rather",200.0],["club",200.0],["machine",200.0],["mpn",80.0],["shortcuts",400.0],["astronomers",400.0],["perimeter",400.0],["pins",400.0],["precisiondrawings",400.0],["profile",400.0],["building",100.0],["specifications",400.0],["soundtrack",200.0],["director",200.0],["discovering",400.0],["date",50.0],["geared",400.0],["02322",400.0],["950000",400.0],["status",400.0],["watercolor",200.0],["four",66.66666666666667],["postfix",400.0],["tale",200.0],["provide",33.333333333333336],["mermaids",400.0],["interior",400.0],["sc",400.0],["increase",66.66666666666667],["biographies",400.0],["operations",80.0],["exotic",133.33333333333334],["circulatory",400.0],["react",200.0],["hierarchy",400.0],["symbols",400.0],["dv",133.33333333333334],["netw",400.0],["24",57.142857142857146],["delay",400.0],["personal",20.0],["defends",400.0],["better",30.76923076923077],["feel",400.0],["policy",80.0],["release",133.33333333333334],["versus",400.0],["harmful",400.0],["food",400.0],["complements",400.0],["scaling",400.0],["inch",100.0],["front",200.0],["shadows",400.0],["ue",400.0],["modp",400.0],["edit",36.36363636363637],["possibilities",80.0],["skills",18.181818181818183],["ints3",400.0],["shores",400.0],["crossing",400.0],["sketches",200.0],["wheel",100.0],["metrics",400.0],["jones",400.0],["navi",400.0],["universalis",400.0],["challenges",80.0],["frames",100.0],["loops",400.0],["bosendorfer",400.0],["album",400.0],["leave",200.0],["mickey",400.0],["zetafax",400.0],["velociraptor",400.0],["blu",200.0],["jump",133.33333333333334],["toolbars",400.0],["emulate",400.0],["adv",200.0],["print",21.05263157894737],["fun",20.0],["eco",400.0],["pleasant",400.0],["small",36.36363636363637],["included",57.142857142857146],["carb",200.0],["wall",400.0],["revolutionary",400.0],["driven",400.0],["reinforces",400.0],["put",133.33333333333334],["motion",66.66666666666667],["introduction",400.0],["corrector",400.0],["distinguish",400.0],["oper",400.0],["info",400.0],["grade",100.0],["diana",400.0],["imagemanager",400.0],["aggregate",400.0],["plants",400.0],["preparation",200.0],["carry",200.0],["aperture",400.0],["sensiive",400.0],["1a",400.0],["bonus",133.33333333333334],["subnetworks",200.0],["europe",200.0],["anyone",66.66666666666667],["nbs",400.0],["exhilarating",400.0],["trend",400.0],["clicking",400.0],["filter",400.0],["arthate",400.0],["1999",400.0],["rolling",400.0],["synthetic",400.0],["cocktails",100.0],["60",133.33333333333334],["pb",400.0],["broadcast",400.0],["notices",400.0],["cook",400.0],["push",400.0],["refund",400.0],["fats",400.0],["pristine",200.0],["customer",133.33333333333334],["11",40.0],["connections",200.0],["datawatch",400.0],["travels",400.0],["softgrid",400.0],["keeper",400.0],["spending",400.0],["recording",133.33333333333334],["cards",36.36363636363637],["states",400.0],["notecards",133.33333333333334],["viva",200.0],["pre",50.0],["hotspots",200.0],["viewer",400.0],["textures",133.33333333333334],["enabling",400.0],["uniquely",400.0],["oracle",400.0],["wysiwyg",200.0],["download",200.0],["sypware",400.0],["need",10.526315789473685],["facing",200.0],["sell",400.0],["printshop",100.0],["presents",50.0],["efficient",57.142857142857146],["note",133.33333333333334],["hr",400.0],["transmit",400.0],["2d",200.0],["folders",100.0],["even",14.285714285714286],["printable",66.66666666666667],["satisfy",400.0],["depths",400.0],["furious",400.0],["itandquot",400.0],["mansion",400.0],["device",100.0],["acquired",400.0],["nonprofits",200.0],["wireless",100.0],["resident",400.0],["5u",400.0],["mobile",66.66666666666667],["traditional",133.33333333333334],["giving",100.0],["keywords",400.0],["sierra",200.0],["caching",400.0],["majestic",200.0],["shop",57.142857142857146],["erasing",400.0],["lotus",400.0],["administrators",133.33333333333334],["russian",400.0],["decorative",400.0],["stateful",400.0],["dorothy",400.0],["impact",200.0],["rerecord",400.0],["tier",400.0],["charges",400.0],["mysterious",200.0],["implementation",133.33333333333334],["terminal",400.0],["textbridge",133.33333333333334],["dodge",400.0],["count",133.33333333333334],["predict",400.0],["quizzes",400.0],["libraries",400.0],["installs",200.0],["pocket",400.0],["illustrations",80.0],["report",100.0],["indragon",400.0],["intensive",400.0],["intego",400.0],["equisys",400.0],["rfc",400.0],["19",400.0],["intervened",400.0],["gulch",400.0],["exercises",200.0],["employee",200.0],["analytical",400.0],["remains",400.0],["00320289",400.0],["preserve",200.0],["nfsv4",400.0],["authorize",400.0],["detection",400.0],["facts",400.0],["overstocking",400.0],["closer",200.0],["planet",200.0],["knowing",400.0],["ways",50.0],["weapons",400.0],["case",33.333333333333336],["integrate",400.0],["novastor",400.0],["affordable",80.0],["perfect",40.0],["tricky",400.0],["128",57.142857142857146],["guitar",28.571428571428573],["dts",400.0],["consists",400.0],["interface",18.181818181818183],["separately",133.33333333333334],["professionalto",400.0],["admin",200.0],["tlc",400.0],["legion",400.0],["objective",400.0],["swim",400.0],["corrupt",400.0],["load",200.0],["detailed",100.0],["color",21.05263157894737],["geometry",400.0],["offerings",200.0],["apache",400.0],["h32",400.0],["wider",200.0],["platinum",133.33333333333334],["forms",36.36363636363637],["leads",400.0],["addition",57.142857142857146],["twain",400.0],["ai",200.0],["tutorials",200.0],["garageband",400.0],["dots",400.0],["donating",400.0],["tedium",400.0],["ghost",400.0],["fire",66.66666666666667],["laptops",133.33333333333334],["gms",133.33333333333334],["atlas",400.0],["immersive",400.0],["quick",80.0],["sound",50.0],["carbohydrate",400.0],["solid",133.33333333333334],["vrs",400.0],["multilayer",400.0],["rope",200.0],["technical",133.33333333333334],["bsendorfer",400.0],["win32",200.0],["campaigns",400.0],["setuid",400.0],["1094",400.0],["vol",400.0],["standby",400.0],["ii",80.0],["ready",50.0],["feed",400.0],["cmyk",400.0],["edr",400.0],["perform",57.142857142857146],["roar",400.0],["member",400.0],["rules",133.33333333333334],["9",44.44444444444444],["complexities",400.0],["development",30.76923076923077],["efx",400.0],["calculation",400.0],["keys",133.33333333333334],["blue",200.0],["producers",400.0],["heterogeneous",400.0],["helped",200.0],["haunting",200.0],["audit",400.0],["commemorate",400.0],["maestro",400.0],["minimize",400.0],["county",400.0],["thought",133.33333333333334],["mia",133.33333333333334],["1",8.51063829787234],["services",30.76923076923077],["coordinate",400.0],["wrangling",400.0],["seasonal",200.0],["lowers",200.0],["moments",200.0],["reproductions",400.0],["engine",100.0],["myinvoices",400.0],["govt",133.33333333333334],["fogware",200.0],["llc",400.0],["touch",400.0],["hacker",400.0],["taken",400.0],["letting",400.0],["kaufman",400.0],["beginners",200.0],["indu",400.0],["enthusiast",400.0],["thinking",133.33333333333334],["300",133.33333333333334],["signals",400.0],["firewall",100.0],["512mb",400.0],["snap",80.0],["throne",400.0],["fontware",400.0],["styles",100.0],["mystery",133.33333333333334],["read",80.0],["successful",200.0],["early",80.0],["finding",400.0],["press",100.0],["individually",400.0],["usually",100.0],["xsr",400.0],["brought",400.0],["satisfaction",400.0],["postcard",200.0],["pet",200.0],["specific",66.66666666666667],["turbocad",200.0],["popular",44.44444444444444],["millies",400.0],["healthy",400.0],["accessible",133.33333333333334],["node",133.33333333333334],["firebox",400.0],["sends",400.0],["prototyping",400.0],["cs",400.0],["cd374",400.0],["images",33.333333333333336],["happens",400.0],["shapes",133.33333333333334],["thrilled",400.0],["gamecard",400.0],["broadband",400.0],["absolutely",400.0],["multiplication",400.0],["witty",400.0],["customize",66.66666666666667],["00034",400.0],["capability",200.0],["locate",200.0],["erased",400.0],["seconds",133.33333333333334],["26162",400.0],["respected",400.0],["passive",400.0],["assemble",400.0],["oversee",400.0],["plug",100.0],["anatomy",200.0],["gifts",400.0],["attacks",80.0],["compatible",44.44444444444444],["monthly",400.0],["favourite",400.0],["warranty",133.33333333333334],["uncompromised",400.0],["real",23.529411764705884],["roofs",400.0],["administer",400.0],["variable",133.33333333333334],["elementary",200.0],["sized",200.0],["managing",100.0],["wherabouts",400.0],["interactivity",200.0],["espro",400.0],["artifacts",133.33333333333334],["available",33.333333333333336],["column",400.0],["generation",200.0],["biggest",200.0],["krupa",400.0],["function",57.142857142857146],["project",30.76923076923077],["120",400.0],["opt",400.0],["delivery",133.33333333333334],["films",200.0],["cs3k",400.0],["active",100.0],["oxford",133.33333333333334],["freddi",133.33333333333334],["engaging",133.33333333333334],["whether",26.666666666666668],["access",20.0],["clips",80.0],["windows",8.51063829787234],["oom",80.0],["retaining",200.0],["continuously",400.0],["diagnose",400.0],["website",400.0],["units",200.0],["taste",200.0],["globally",400.0],["substantially",400.0],["kingdom",400.0],["importing",400.0],["decipher",400.0],["phantom",400.0],["acceleration",400.0],["improving",400.0],["pz24a0075",400.0],["water",133.33333333333334],["preventing",400.0],["started",200.0],["1788",400.0],["test",57.142857142857146],["ruthless",200.0],["scored",400.0],["category",200.0],["may",50.0],["students",33.333333333333336],["packaging",400.0],["helping",133.33333333333334],["town",100.0],["hardware",100.0],["track",57.142857142857146],["absorbing",400.0],["jvc",400.0],["soundblaster",400.0],["timing",400.0],["livesecurity",400.0],["searches",200.0],["assume",400.0],["backed",80.0],["2002",133.33333333333334],["frees",200.0],["write",133.33333333333334],["week",400.0],["directory",200.0],["receive",133.33333333333334],["limited",100.0],["positions",400.0],["battlefields",400.0],["breadth",400.0],["caused",400.0],["recapturing",400.0],["resisted",400.0],["follow",44.44444444444444],["conversation",400.0],["equipment",200.0],["everything",28.571428571428573],["pedalling",400.0],["april",400.0],["8ghz",400.0],["james",400.0],["0",8.695652173913043],["scratch",100.0],["suite",14.814814814814815],["activities",25.0],["hdv",200.0],["wayfarers",400.0],["removing",400.0],["pbupgrd",400.0],["heard",200.0],["replication",400.0],["literally",200.0],["formula",400.0],["affect",200.0],["entire",44.44444444444444],["desserts",400.0],["turns",200.0],["ax",400.0],["item",400.0],["progress",100.0],["car",400.0],["enhance",80.0],["entourage",400.0],["eagle",200.0],["phones",400.0],["among",400.0],["elegant",400.0],["subtle",400.0],["4x6",200.0],["assures",400.0],["requiring",400.0],["distiller",100.0],["essence",400.0],["sun",133.33333333333334],["suv",400.0],["gaming",400.0],["booklet",200.0],["international",133.33333333333334],["supporting",400.0],["partners",66.66666666666667],["radicals",400.0],["velocity",400.0],["web",14.285714285714286],["making",80.0],["sing",133.33333333333334],["1024",133.33333333333334],["shared",400.0],["publisher",66.66666666666667],["canuse",400.0],["division",400.0],["iphoto",400.0],["32",100.0],["optimal",400.0],["640",400.0],["readiris",400.0],["installer",400.0],["500mb",400.0],["military",200.0],["as400",400.0],["purchases",400.0],["win",8.88888888888889],["v22",200.0],["visualization",400.0],["newsfeeds",400.0],["unpredictable",400.0],["apply",100.0],["intranet",400.0],["graphs",400.0],["interpretair",200.0],["positive",400.0],["h",200.0],["scope",400.0],["sexy",400.0],["los",400.0],["charcoal",200.0],["definition",133.33333333333334],["micro",200.0],["oscar",200.0],["synchronize",200.0],["phone",133.33333333333334],["bellarion",400.0],["teaching",66.66666666666667],["90",133.33333333333334],["flawless",400.0],["150",133.33333333333334],["organizations",50.0],["frequently",400.0],["drawer",400.0],["ed",200.0],["science",66.66666666666667],["strain",200.0],["movies",66.66666666666667],["intranets",400.0],["webster",400.0],["spy",133.33333333333334],["1pk",400.0],["laurence",400.0],["kobe",400.0],["synchronizes",200.0],["mayhem",400.0],["phrase",200.0],["icopydvds2",133.33333333333334],["information",12.5],["coursewarecustomizing",400.0],["turn",28.571428571428573],["softwaretools",400.0],["640x480",200.0],["familiar",100.0],["cd08393wi",400.0],["arswaargh",400.0],["3des",400.0],["bags",400.0],["broad",133.33333333333334],["dinosaurs",200.0],["independent",200.0],["portfolio",400.0],["records",200.0],["guesswork",200.0],["jumpstart",80.0],["fleet",400.0],["purchase",80.0],["organization",80.0],["famous",200.0],["skeletal",400.0],["light",133.33333333333334],["dramatica",200.0],["introducing",400.0],["webroot",200.0],["cannon",400.0],["things",80.0],["solos",200.0],["firehouse",400.0],["exclusive",133.33333333333334],["clubhouse",400.0],["developed",100.0],["kyoto",400.0],["course",133.33333333333334],["sony",133.33333333333334],["captions",200.0],["adhesive",400.0],["position",400.0],["scrapbook",400.0],["maps",400.0],["pcs",100.0],["platform",44.44444444444444],["tiff",400.0],["mail",50.0],["freely",400.0],["endor",400.0],["flex",200.0],["smarter",400.0],["band",133.33333333333334],["party",80.0],["dealing",400.0],["environmental",400.0],["easily",15.384615384615385],["fantasy",200.0],["wrkgrp",400.0],["lifetime",400.0],["realistic",100.0],["25",200.0],["upg",33.333333333333336],["upsell",400.0],["mercy",400.0],["improvise",400.0],["meets",200.0],["series",50.0],["society",400.0],["thoughtfully",200.0],["logo",200.0],["brings",66.66666666666667],["living",133.33333333333334],["sha",400.0],["culture",400.0],["god",200.0],["re",21.05263157894737],["3gp",400.0],["got",66.66666666666667],["strategies",80.0],["foundation",200.0],["found",200.0],["timeline",200.0],["garnishes",400.0],["binary",400.0],["houses",400.0],["uncover",400.0],["poems",400.0],["r5",400.0],["sp1",100.0],["applications",17.391304347826086],["lfl",400.0],["phrases",200.0],["pictures",66.66666666666667],["confident",400.0],["todayandquot",400.0],["thing",133.33333333333334],["person",400.0],["leading",40.0],["timelines",133.33333333333334],["gif",400.0],["ste",400.0],["flood",400.0],["kernel",200.0],["dungeons",400.0],["intuitively",400.0],["acoustic",400.0],["fi",400.0],["csdc",133.33333333333334],["loaded",200.0],["ferris",200.0],["unknown",200.0],["player",66.66666666666667],["needed",100.0],["flashcards",400.0],["toolset",133.33333333333334],["west",400.0],["enforcement",200.0],["corporations",400.0],["collectible",400.0],["1419",400.0],["wav",400.0],["x3",200.0],["wide",57.142857142857146],["quantitative",400.0],["installers",200.0],["r3900292",400.0],["prevent",133.33333333333334],["double",200.0],["players",57.142857142857146],["thinkyou",400.0],["country",200.0],["bridge",100.0],["hijackers",400.0],["exploring",400.0],["portraits",400.0],["98",22.22222222222222],["raw",133.33333333333334],["close",400.0],["portrait",400.0],["seen",400.0],["ratings",400.0],["patterns",400.0],["convenient",66.66666666666667],["mix",57.142857142857146],["articles",200.0],["commissions",400.0],["superior",100.0],["floods",400.0],["communications",26.666666666666668],["200",100.0],["headset",400.0],["quote",200.0],["thoroughly",400.0],["simplify",400.0],["x",12.903225806451612],["photofinishing",400.0],["comparison",200.0],["mypc",400.0],["menus",66.66666666666667],["studyworks",400.0],["doc",400.0],["riddled",400.0],["fat16",400.0],["itempowers",400.0],["occasion",200.0],["imaginative",400.0],["incorporate",200.0],["roleplaying",400.0],["pamphlets",200.0],["orleans",400.0],["gates",400.0],["withstand",400.0],["text",25.0],["league",200.0],["bring",33.333333333333336],["babwbr1151s05",400.0],["earth",400.0],["sleuths",200.0],["favor",200.0],["menu",400.0],["darklords",400.0],["cultures",400.0],["entities",200.0],["simplicity",400.0],["solitary",400.0],["gods",100.0],["8",18.181818181818183],["x3mac",400.0],["loved",400.0],["ip",400.0],["loss",400.0],["dvds",28.571428571428573],["professionals",50.0],["assistant",400.0],["postscript",200.0],["20033",400.0],["p",200.0],["ssl",400.0],["xvid",200.0],["fingerpicking",200.0],["cuisine",200.0],["pk",133.33333333333334],["reveal",400.0],["ended",400.0],["fields",100.0],["crosspicking",400.0],["twice",400.0],["wish",133.33333333333334],["tuner",200.0],["mito",400.0],["characteristics",400.0],["enhances",400.0],["solo",200.0],["intelligence",133.33333333333334],["routers",200.0],["eidos",200.0],["edition",12.5],["equip",400.0],["b26",200.0],["collects",400.0],["pastel",133.33333333333334],["nis",400.0],["clickable",400.0],["eurotalk",400.0],["movie",44.44444444444444],["discounts",400.0],["joy",200.0],["import",36.36363636363637],["fs2004",400.0],["pkcs",400.0],["converting",400.0],["tables",133.33333333333334],["scanning",100.0],["comp",200.0],["prior",200.0],["efficiency",100.0],["invest",400.0],["highlight",400.0],["approval",400.0],["qual",400.0],["foes",400.0],["mob",400.0],["mcafee",400.0],["many",25.0],["forced",400.0],["0c",133.33333333333334],["retiming",400.0],["agile",400.0],["attributes",133.33333333333334],["affairs",200.0],["mini",200.0],["relational",400.0],["overview",22.22222222222222],["scrapbooks",200.0],["amazingly",200.0],["connectivity",80.0],["adding",200.0],["framework",100.0],["path",100.0],["table",400.0],["vrs4",400.0],["equips",400.0],["instrument",200.0],["encryption",100.0],["submarine",400.0],["imports",400.0],["linguistic",400.0],["rfc2054",400.0],["ethernet",400.0],["sending",400.0],["quantities",400.0],["downed",400.0],["new",6.896551724137931],["photos",26.666666666666668],["unified",200.0],["clusters",400.0]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"token","type":"\"string\"","metadata":"{}"},{"name":"value","type":"\"double\"","metadata":"{}"}],"overflow":true,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482482189367E12,"submitTime":1.482482178247E12,"finishTime":1.482482189719E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"barChart","width":"810","height":"326","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ca90e285-d97c-4b07-85b5-49c44cbe4f33"},{"version":"CommandV1","origId":2216104459508912,"guid":"81dcebd3-8480-4020-9266-f2e73ff53286","subtype":"command","commandType":"auto","position":41.0,"command":"%md\n#### **(2f) Implement a TF-IDF function**\nUse your `tf` function to implement a `tfidf(tokens, idfs)` function that takes a list of tokens from a document and a Python dictionary of IDF weights and returns a Python dictionary mapping individual tokens to total TF-IDF weights.\n \nThe steps your function should perform are:\n* Calculate the token frequencies (TF) for `tokens`\n* Create a Python dictionary where each token maps to the token's frequency times the token's IDF weight\n \nUse your `tfidf` function to compute the weights of Amazon product record 'b000hkgj8k'. To do this, we need to extract the record for the token from the tokenized small Amazon dataset and we need to convert the IDFs for the small dataset into a Python dictionary. We can do the first part, by using a `filter()` transformation to extract the matching record and a `collect()` action to return the value to the driver.\n \nFor the second part, we use the [`collectAsMap()` action](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collectAsMap) to return the IDFs to the driver as a Python dictionary.","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482178265E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b4552a9e-2117-4731-b410-e76b1ccb20c6"},{"version":"CommandV1","origId":2216104459508913,"guid":"fc2f34fb-327d-4845-9060-012f04ded344","subtype":"command","commandType":"auto","position":42.0,"command":"# TODO: Replace <FILL IN> with appropriate code\ndef tfidf(tokens, idfs):\n    \"\"\" Compute TF-IDF\n    Args:\n        tokens (list of str): input list of tokens from tokenize\n        idfs (dictionary): record to IDF value\n    Returns:\n        dictionary: a dictionary of records to TF-IDF values\n    \"\"\"\n    tfs = tf(tokens)\n    tfIdfDict = dict([ (key, val * idfs.get(key)) for key, val in tfs.items()])\n    return tfIdfDict\n\nrecb000hkgj8k = amazonRecToToken.filter(lambda x: x[0] == 'b000hkgj8k').collect()[0][1]\nidfsSmallWeights = idfsSmall.collectAsMap()\nrec_b000hkgj8k_weights = tfidf(recb000hkgj8k, idfsSmallWeights)\n\nprint 'Amazon record \"b000hkgj8k\" has tokens and weights:\\n%s' % rec_b000hkgj8k_weights","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Amazon record &quot;b000hkgj8k&quot; has tokens and weights:\n{&apos;autocad&apos;: 33.33333333333333, &apos;autodesk&apos;: 8.333333333333332, &apos;courseware&apos;: 66.66666666666666, &apos;psg&apos;: 33.33333333333333, &apos;2007&apos;: 3.5087719298245617, &apos;customizing&apos;: 16.666666666666664, &apos;interface&apos;: 3.0303030303030303}\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482189737E12,"submitTime":1.482482178284E12,"finishTime":1.482482190339E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"7628cf17-fdf7-466d-86b1-e4f2e47a16b7"},{"version":"CommandV1","origId":2216104459508914,"guid":"e815db80-0c91-4f2d-920d-eb28f3bf4ac2","subtype":"command","commandType":"auto","position":43.0,"command":"# TEST Implement a TF-IDF function (2f)\nTest.assertEquals(rec_b000hkgj8k_weights,\n                   {'autocad': 33.33333333333333, 'autodesk': 8.333333333333332,\n                    'courseware': 66.66666666666666, 'psg': 33.33333333333333,\n                    '2007': 3.5087719298245617, 'customizing': 16.666666666666664,\n                    'interface': 3.0303030303030303}, 'incorrect rec_b000hkgj8k_weights')","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">1 test passed.\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482190358E12,"submitTime":1.482482178296E12,"finishTime":1.482482190388E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"2593e9ba-5192-4e0d-b083-d925506685fb"},{"version":"CommandV1","origId":2216104459508915,"guid":"4cf67377-85ec-4a07-96f8-0773a81b7552","subtype":"command","commandType":"auto","position":44.0,"command":"%md\n#### **Part 3: ER as Text Similarity - Cosine Similarity**\nNow we are ready to do text comparisons in a formal way. The metric of string distance we will use is called **[cosine similarity][cosine]**. We will treat each document as a vector in some high dimensional space. Then, to compare two documents we compute the cosine of the angle between their two document vectors. This is *much* easier than it sounds.\n \nThe first question to answer is how do we represent documents as vectors? The answer is familiar: bag-of-words! We treat each unique token as a dimension, and treat token weights as magnitudes in their respective token dimensions. For example, suppose we use simple counts as weights, and we want to interpret the string \"Hello, world!  Goodbye, world!\" as a vector. Then in the \"hello\" and \"goodbye\" dimensions the vector has value 1, in the \"world\" dimension it has value 2, and it is zero in all other dimensions.\n \nThe next question is: given two vectors how do we find the cosine of the angle between them? Recall the formula for the dot product of two vectors:\n\\\\[ a \\cdot b = \\| a \\| \\| b \\| \\cos \\theta \\\\]\nHere \\\\( a \\cdot b = \\sum a_i b_i \\\\) is the ordinary dot product of two vectors, and \\\\( \\|a\\| = \\sqrt{ \\sum a_i^2 } \\\\) is the norm of \\\\( a \\\\).\n \nWe can rearrange terms and solve for the cosine to find it is simply the normalized dot product of the vectors. With our vector model, the dot product and norm computations are simple functions of the bag-of-words document representations, so we now have a formal way to compute similarity:\n\\\\[ similarity = \\cos \\theta = \\frac{a \\cdot b}{\\|a\\| \\|b\\|} = \\frac{\\sum a_i b_i}{\\sqrt{\\sum a_i^2} \\sqrt{\\sum b_i^2}} \\\\]\n \nSetting aside the algebra, the geometric interpretation is more intuitive. The angle between two document vectors is small if they share many tokens in common, because they are pointing in roughly the same direction. For that case, the cosine of the angle will be large. Otherwise, if the angle is large (and they have few words in common), the cosine is small. Therefore, cosine similarity scales proportionally with our intuitive sense of similarity.\n[cosine]: https://en.wikipedia.org/wiki/Cosine_similarity","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482178308E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ff18f4ed-6f90-4fcd-89dc-b09203a177d5"},{"version":"CommandV1","origId":2216104459508916,"guid":"acba7dac-1917-447c-a7b6-342fd18cdc3f","subtype":"command","commandType":"auto","position":45.0,"command":"%md\n#### **(3a) Implement the components of a `cosineSimilarity` function**\nImplement the components of a `cosineSimilarity` function.\nUse the `tokenize` and `tfidf` functions, and the IDF weights from Part 2 for extracting tokens and assigning them weights.\nThe steps you should perform are:\n* Define a function `dotprod` that takes two Python dictionaries and produces the dot product of them, where the dot product is defined as the sum of the product of values for tokens that appear in *both* dictionaries\n* Define a function `norm` that returns the square root of the dot product of a dictionary and itself\n* Define a function `cossim` that returns the dot product of two dictionaries divided by the norm of the first dictionary and then by the norm of the second dictionary","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482178326E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9ac8fe6a-76f5-4d87-a194-e300f76a0e26"},{"version":"CommandV1","origId":2216104459508917,"guid":"e93fdd64-9cf0-4a96-a535-54db7fb615f0","subtype":"command","commandType":"auto","position":46.0,"command":"# TODO: Replace <FILL IN> with appropriate code\nimport math\n\ndef dotprod(a, b):\n    \"\"\" Compute dot product\n    Args:\n        a (dictionary): first dictionary of record to value\n        b (dictionary): second dictionary of record to value\n    Returns:\n        dotProd: result of the dot product with the two input dictionaries\n    \"\"\"\n    return sum([a[ak] * b[ak] for ak in a if ak in b])\n\ndef norm(a):\n    \"\"\" Compute square root of the dot product\n    Args:\n        a (dictionary): a dictionary of record to value\n    Returns:\n        norm (float): the square root of the dot product value\n    \"\"\"\n    return math.sqrt(dotprod(a, a))\n\ndef cossim(a, b):\n    \"\"\" Compute cosine similarity\n    Args:\n        a (dictionary): first dictionary of record to value\n        b (dictionary): second dictionary of record to value\n    Returns:\n        cossim: dot product of two dictionaries divided by the norm of the first dictionary and\n                then by the norm of the second dictionary\n    \"\"\"\n    return dotprod(a, b)/math.sqrt(dotprod(a,a))/math.sqrt(dotprod(b,b))\n\ntestVec1 = {'foo': 2, 'bar': 3, 'baz': 5 }\ntestVec2 = {'foo': 1, 'bar': 0, 'baz': 20 }\ndp = dotprod(testVec1, testVec2)\nnm = norm(testVec1)\nprint dp, nm","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">102 6.16441400297\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482190401E12,"submitTime":1.482482178343E12,"finishTime":1.482482190454E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"410636f0-2f00-493e-8935-56ddb42bf7f5"},{"version":"CommandV1","origId":2216104459508918,"guid":"dbab4327-6e18-4511-85a8-dd6d71087849","subtype":"command","commandType":"auto","position":47.0,"command":"# TEST Implement the components of a cosineSimilarity function (3a)\nTest.assertEquals(dp, 102, 'incorrect dp')\nTest.assertTrue(abs(nm - 6.16441400297) < 0.0000001, 'incorrrect nm')","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">1 test passed.\n1 test passed.\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482190467E12,"submitTime":1.482482178354E12,"finishTime":1.482482190495E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"036dc3a9-71ed-4092-8890-af95728aa474"},{"version":"CommandV1","origId":2216104459508919,"guid":"c7cef2dc-c3c0-4ac0-bcbd-7e1623d8b5d8","subtype":"command","commandType":"auto","position":48.0,"command":"%md\n#### **(3b) Implement a `cosineSimilarity` function**\nImplement a `cosineSimilarity(string1, string2, idfsDictionary)` function that takes two strings and a dictionary of IDF weights, and computes their cosine similarity in the context of some global IDF weights.\n \nThe steps you should perform are:\n* Apply your `tfidf` function to the tokenized first and second strings, using the dictionary of IDF weights\n* Compute and return your `cossim` function applied to the results of the two `tfidf` functions","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482178366E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a7f67744-08ec-42c2-93e4-6da1f668e360"},{"version":"CommandV1","origId":2216104459508920,"guid":"d3a23ab8-432f-4f26-93d9-015180e24477","subtype":"command","commandType":"auto","position":49.0,"command":"# TODO: Replace <FILL IN> with appropriate code\ndef cosineSimilarity(string1, string2, idfsDictionary):\n    \"\"\" Compute cosine similarity between two strings\n    Args:\n        string1 (str): first string\n        string2 (str): second string\n        idfsDictionary (dictionary): a dictionary of IDF values\n    Returns:\n        cossim: cosine similarity value\n    \"\"\"\n    w1 = tfidf(tokenize(string1), idfsDictionary)\n    w2 = tfidf(tokenize(string2), idfsDictionary)\n    return cossim(w1, w2)\n\ncossimAdobe = cosineSimilarity('Adobe Photoshop',\n                               'Adobe Illustrator',\n                               idfsSmallWeights)\n\nprint cossimAdobe","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">0.0577243382163\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<span class=\"ansired\">TypeError</span>: norm() takes exactly 1 argument (2 given)","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-66-d9c3d83350ed&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     15</span> cossimAdobe = cosineSimilarity(&apos;Adobe Photoshop&apos;,\n<span class=\"ansigreen\">     16</span>                                <span class=\"ansiblue\">&apos;Adobe Illustrator&apos;</span><span class=\"ansiyellow\">,</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 17</span><span class=\"ansiyellow\">                                idfsSmallWeights)\n</span><span class=\"ansigreen\">     18</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     19</span> <span class=\"ansigreen\">print</span> cossimAdobe<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">&lt;ipython-input-66-d9c3d83350ed&gt;</span> in <span class=\"ansicyan\">cosineSimilarity</span><span class=\"ansiblue\">(string1, string2, idfsDictionary)</span>\n<span class=\"ansigreen\">     11</span>     w1 <span class=\"ansiyellow\">=</span> tfidf<span class=\"ansiyellow\">(</span>tokenize<span class=\"ansiyellow\">(</span>string1<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> idfsDictionary<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     12</span>     w2 <span class=\"ansiyellow\">=</span> tfidf<span class=\"ansiyellow\">(</span>tokenize<span class=\"ansiyellow\">(</span>string2<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> idfsDictionary<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 13</span><span class=\"ansiyellow\">     </span><span class=\"ansigreen\">return</span> cossim<span class=\"ansiyellow\">(</span>w1<span class=\"ansiyellow\">,</span> w2<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     14</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     15</span> cossimAdobe = cosineSimilarity(&apos;Adobe Photoshop&apos;,\n\n<span class=\"ansigreen\">&lt;ipython-input-63-a26b93a3a965&gt;</span> in <span class=\"ansicyan\">cossim</span><span class=\"ansiblue\">(a, b)</span>\n<span class=\"ansigreen\">     30</span>                 then by the norm of the second dictionary<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     31</span>     &quot;&quot;&quot;\n<span class=\"ansigreen\">---&gt; 32</span><span class=\"ansiyellow\">     </span><span class=\"ansigreen\">return</span> dotprod<span class=\"ansiyellow\">(</span>a<span class=\"ansiyellow\">,</span> b<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">/</span>math<span class=\"ansiyellow\">.</span>sqrt<span class=\"ansiyellow\">(</span>norm<span class=\"ansiyellow\">(</span>a<span class=\"ansiyellow\">,</span>a<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">/</span>math<span class=\"ansiyellow\">.</span>sqrt<span class=\"ansiyellow\">(</span>norm<span class=\"ansiyellow\">(</span>b<span class=\"ansiyellow\">,</span>b<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     33</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     34</span> testVec1 <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">{</span><span class=\"ansiblue\">&apos;foo&apos;</span><span class=\"ansiyellow\">:</span> <span class=\"ansicyan\">2</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;bar&apos;</span><span class=\"ansiyellow\">:</span> <span class=\"ansicyan\">3</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;baz&apos;</span><span class=\"ansiyellow\">:</span> <span class=\"ansicyan\">5</span> <span class=\"ansiyellow\">}</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">TypeError</span>: norm() takes exactly 1 argument (2 given)\n</div>","workflows":[],"startTime":1.482482190507E12,"submitTime":1.482482178383E12,"finishTime":1.482482191054E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"0b963c04-0526-40e5-959f-f02c6ce3841d"},{"version":"CommandV1","origId":2216104459508921,"guid":"19efd13f-7113-414b-a9c4-3c3bb3a76314","subtype":"command","commandType":"auto","position":50.0,"command":"# TEST Implement a cosineSimilarity function (3b)\nTest.assertTrue(abs(cossimAdobe - 0.0577243382163) < 0.0000001, 'incorrect cossimAdobe')","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">1 test passed.\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482191059E12,"submitTime":1.482482178395E12,"finishTime":1.482482191132E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"76a9db83-9dd7-45b6-9987-77ec803a3dc6"},{"version":"CommandV1","origId":2216104459508922,"guid":"103ca150-bded-40b3-8256-c862bb6963a6","subtype":"command","commandType":"auto","position":51.0,"command":"%md\n#### **(3c) Perform Entity Resolution**\nNow we can finally do some entity resolution!\nFor *every* product record in the small Google dataset, use your `cosineSimilarity` function to compute its similarity to every record in the small Amazon dataset.  Then, build a dictionary mapping `(Google URL, Amazon ID)` tuples to similarity scores between 0 and 1.\nWe'll do this computation two different ways, first we'll do it without a broadcast variable, and then we'll use a broadcast variable\n \nThe steps you should perform are:\n* Create an RDD that is a combination of the small Google and small Amazon datasets that has as elements all pairs of elements (a, b) where a is in self and b is in other. The result will be an RDD of the form: `[ ((Google URL1, Google String1), (Amazon ID1, Amazon String1)), ((Google URL1, Google String1), (Amazon ID2, Amazon String2)), ((Google URL2, Google String2), (Amazon ID1, Amazon String1)), ... ]`\n* Define a worker function that given an element from the combination RDD computes the cosineSimlarity for the two records in the element\n* Apply the worker function to every element in the RDD\n \nNow, compute the similarity between Amazon record `b000o24l3q` and Google record `http://www.google.com/base/feeds/snippets/17242822440574356561`.","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482178407E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"334127a3-0ecf-435b-8486-1ae94e9b23f1"},{"version":"CommandV1","origId":2883926800250254,"guid":"47400ee3-a6f7-4c1e-a707-092a6d568982","subtype":"command","commandType":"auto","position":51.5,"command":"res = crossSmall.map(lambda x : x).take(1)[0]\nres","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">82</span><span class=\"ansired\">]: </span>\n((&apos;http://www.google.com/base/feeds/snippets/11448761432933644608&apos;,\n  &apos;spanish vocabulary builder &quot;expand your vocabulary! contains fun lessons that both teach and entertain you\\&apos;ll quickly find yourself mastering new terms. includes games and more!&quot; &apos;),\n (&apos;b000jz4hqo&apos;,\n  &apos;clickart 950 000 - premier image pack (dvd-rom)  &quot;broderbund&quot;&apos;))\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.48248370681E12,"submitTime":1.482483706838E12,"finishTime":1.482483706931E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"331e51c8-9255-4a58-87c6-2042074989f0"},{"version":"CommandV1","origId":2216104459508923,"guid":"2dc95d5a-d2e6-409e-be01-be6d465ba90f","subtype":"command","commandType":"auto","position":52.0,"command":"# TODO: Replace <FILL IN> with appropriate code\ncrossSmall = (googleSmall\n              .cartesian(amazonSmall)\n              .cache())\n\ndef computeSimilarity(record):\n    \"\"\" Compute similarity on a combination record\n    Args:\n        record: a pair, (google record, amazon record)\n    Returns:\n        pair: a pair, (google URL, amazon ID, cosine similarity value)\n    \"\"\"\n    googleRec = record[0]\n    amazonRec = record[1]\n    googleURL = googleRec[0]\n    amazonID = amazonRec[0]\n    googleValue = googleRec[1]\n    amazonValue = amazonRec[1]\n    cs = cosineSimilarity(googleValue, amazonValue, idfsSmallWeights)\n    return (googleURL, amazonID, cs)\n\nsimilarities = (crossSmall\n                .map(computeSimilarity)\n                .cache())\n\ndef similar(amazonID, googleURL):\n    \"\"\" Return similarity value\n    Args:\n        amazonID: amazon ID\n        googleURL: google URL\n    Returns:\n        similar: cosine similarity value\n    \"\"\"\n    return (similarities\n            .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))\n            .collect()[0][2])\n\nsimilarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')\nprint 'Requested similarity is %s.' % similarityAmazonGoogle","commandVersion":1,"state":"error","results":null,"errorSummary":"<span class=\"ansired\">Exception</span>: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Exception</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-86-a79ef9dcc8f9&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     21</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     22</span> similarities = (crossSmall\n<span class=\"ansigreen\">---&gt; 23</span><span class=\"ansiyellow\">                 </span><span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span>computeSimilarity<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     24</span>                 .cache())\n<span class=\"ansigreen\">     25</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">cache</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    224</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">    225</span>         self<span class=\"ansiyellow\">.</span>is_cached <span class=\"ansiyellow\">=</span> True<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 226</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>persist<span class=\"ansiyellow\">(</span>StorageLevel<span class=\"ansiyellow\">.</span>MEMORY_ONLY<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    227</span>         <span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    228</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">persist</span><span class=\"ansiblue\">(self, storageLevel)</span>\n<span class=\"ansigreen\">    240</span>         self<span class=\"ansiyellow\">.</span>is_cached <span class=\"ansiyellow\">=</span> True<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    241</span>         javaStorageLevel <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>ctx<span class=\"ansiyellow\">.</span>_getJavaStorageLevel<span class=\"ansiyellow\">(</span>storageLevel<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 242</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>_jrdd<span class=\"ansiyellow\">.</span>persist<span class=\"ansiyellow\">(</span>javaStorageLevel<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    243</span>         <span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    244</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">_jrdd</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">   2401</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2402</span>         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n<span class=\"ansigreen\">-&gt; 2403</span><span class=\"ansiyellow\">                                       self._jrdd_deserializer, profiler)\n</span><span class=\"ansigreen\">   2404</span>         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n<span class=\"ansigreen\">   2405</span>                                              self.preservesPartitioning)\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">_wrap_function</span><span class=\"ansiblue\">(sc, func, deserializer, serializer, profiler)</span>\n<span class=\"ansigreen\">   2334</span>     <span class=\"ansigreen\">assert</span> serializer<span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;serializer should not be empty&quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2335</span>     command <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">(</span>func<span class=\"ansiyellow\">,</span> profiler<span class=\"ansiyellow\">,</span> deserializer<span class=\"ansiyellow\">,</span> serializer<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 2336</span><span class=\"ansiyellow\">     </span>pickled_command<span class=\"ansiyellow\">,</span> broadcast_vars<span class=\"ansiyellow\">,</span> env<span class=\"ansiyellow\">,</span> includes <span class=\"ansiyellow\">=</span> _prepare_for_python_RDD<span class=\"ansiyellow\">(</span>sc<span class=\"ansiyellow\">,</span> command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2337</span>     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n<span class=\"ansigreen\">   2338</span>                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">_prepare_for_python_RDD</span><span class=\"ansiblue\">(sc, command)</span>\n<span class=\"ansigreen\">   2313</span>     <span class=\"ansired\"># the serialized command will be compressed by broadcast</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2314</span>     ser <span class=\"ansiyellow\">=</span> CloudPickleSerializer<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 2315</span><span class=\"ansiyellow\">     </span>pickled_command <span class=\"ansiyellow\">=</span> ser<span class=\"ansiyellow\">.</span>dumps<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2316</span>     <span class=\"ansigreen\">if</span> len<span class=\"ansiyellow\">(</span>pickled_command<span class=\"ansiyellow\">)</span> <span class=\"ansiyellow\">&gt;</span> <span class=\"ansiyellow\">(</span><span class=\"ansicyan\">1</span> <span class=\"ansiyellow\">&lt;&lt;</span> <span class=\"ansicyan\">20</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span>  <span class=\"ansired\"># 1M</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2317</span>         <span class=\"ansired\"># The broadcast will have same life cycle as created PythonRDD</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/serializers.py</span> in <span class=\"ansicyan\">dumps</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    426</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    427</span>     <span class=\"ansigreen\">def</span> dumps<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 428</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> cloudpickle<span class=\"ansiyellow\">.</span>dumps<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">2</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    429</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    430</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">dumps</span><span class=\"ansiblue\">(obj, protocol)</span>\n<span class=\"ansigreen\">    655</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    656</span>     cp <span class=\"ansiyellow\">=</span> CloudPickler<span class=\"ansiyellow\">(</span>file<span class=\"ansiyellow\">,</span>protocol<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 657</span><span class=\"ansiyellow\">     </span>cp<span class=\"ansiyellow\">.</span>dump<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    658</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    659</span>     <span class=\"ansigreen\">return</span> file<span class=\"ansiyellow\">.</span>getvalue<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">dump</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    105</span>         self<span class=\"ansiyellow\">.</span>inject_addons<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    106</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 107</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> Pickler<span class=\"ansiyellow\">.</span>dump<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    108</span>         <span class=\"ansigreen\">except</span> RuntimeError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    109</span>             <span class=\"ansigreen\">if</span> <span class=\"ansiblue\">&apos;recursion&apos;</span> <span class=\"ansigreen\">in</span> e<span class=\"ansiyellow\">.</span>args<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">dump</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    222</span>         <span class=\"ansigreen\">if</span> self<span class=\"ansiyellow\">.</span>proto <span class=\"ansiyellow\">&gt;=</span> <span class=\"ansicyan\">2</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    223</span>             self<span class=\"ansiyellow\">.</span>write<span class=\"ansiyellow\">(</span>PROTO <span class=\"ansiyellow\">+</span> chr<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>proto<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 224</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>save<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    225</span>         self<span class=\"ansiyellow\">.</span>write<span class=\"ansiyellow\">(</span>STOP<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    226</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save_tuple</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    566</span>         write<span class=\"ansiyellow\">(</span>MARK<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    567</span>         <span class=\"ansigreen\">for</span> element <span class=\"ansigreen\">in</span> obj<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 568</span><span class=\"ansiyellow\">             </span>save<span class=\"ansiyellow\">(</span>element<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    569</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    570</span>         <span class=\"ansigreen\">if</span> id<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">in</span> memo<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function</span><span class=\"ansiblue\">(self, obj, name)</span>\n<span class=\"ansigreen\">    202</span>             klass <span class=\"ansiyellow\">=</span> getattr<span class=\"ansiyellow\">(</span>themodule<span class=\"ansiyellow\">,</span> name<span class=\"ansiyellow\">,</span> None<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    203</span>             <span class=\"ansigreen\">if</span> klass <span class=\"ansigreen\">is</span> None <span class=\"ansigreen\">or</span> klass <span class=\"ansigreen\">is</span> <span class=\"ansigreen\">not</span> obj<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 204</span><span class=\"ansiyellow\">                 </span>self<span class=\"ansiyellow\">.</span>save_function_tuple<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    205</span>                 <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    206</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function_tuple</span><span class=\"ansiblue\">(self, func)</span>\n<span class=\"ansigreen\">    239</span>         <span class=\"ansired\"># create a skeleton function object and memoize it</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    240</span>         save<span class=\"ansiyellow\">(</span>_make_skel_func<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 241</span><span class=\"ansiyellow\">         </span>save<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">(</span>code<span class=\"ansiyellow\">,</span> closure<span class=\"ansiyellow\">,</span> base_globals<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    242</span>         write<span class=\"ansiyellow\">(</span>pickle<span class=\"ansiyellow\">.</span>REDUCE<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    243</span>         self<span class=\"ansiyellow\">.</span>memoize<span class=\"ansiyellow\">(</span>func<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save_tuple</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    552</span>         <span class=\"ansigreen\">if</span> n <span class=\"ansiyellow\">&lt;=</span> <span class=\"ansicyan\">3</span> <span class=\"ansigreen\">and</span> proto <span class=\"ansiyellow\">&gt;=</span> <span class=\"ansicyan\">2</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    553</span>             <span class=\"ansigreen\">for</span> element <span class=\"ansigreen\">in</span> obj<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 554</span><span class=\"ansiyellow\">                 </span>save<span class=\"ansiyellow\">(</span>element<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    555</span>             <span class=\"ansired\"># Subtle.  Same as in the big comment below.</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    556</span>             <span class=\"ansigreen\">if</span> id<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">in</span> memo<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save_list</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    604</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    605</span>         self<span class=\"ansiyellow\">.</span>memoize<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 606</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>_batch_appends<span class=\"ansiyellow\">(</span>iter<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    607</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    608</span>     dispatch<span class=\"ansiyellow\">[</span>ListType<span class=\"ansiyellow\">]</span> <span class=\"ansiyellow\">=</span> save_list<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">_batch_appends</span><span class=\"ansiblue\">(self, items)</span>\n<span class=\"ansigreen\">    640</span>                 write<span class=\"ansiyellow\">(</span>APPENDS<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    641</span>             <span class=\"ansigreen\">elif</span> n<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 642</span><span class=\"ansiyellow\">                 </span>save<span class=\"ansiyellow\">(</span>tmp<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    643</span>                 write<span class=\"ansiyellow\">(</span>APPEND<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    644</span>             <span class=\"ansired\"># else tmp is empty, and we&apos;re done</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function</span><span class=\"ansiblue\">(self, obj, name)</span>\n<span class=\"ansigreen\">    196</span>         <span class=\"ansigreen\">if</span> islambda<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">or</span> obj<span class=\"ansiyellow\">.</span>__code__<span class=\"ansiyellow\">.</span>co_filename <span class=\"ansiyellow\">==</span> <span class=\"ansiblue\">&apos;&lt;stdin&gt;&apos;</span> <span class=\"ansigreen\">or</span> themodule <span class=\"ansigreen\">is</span> None<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    197</span>             <span class=\"ansired\">#print(&quot;save global&quot;, islambda(obj), obj.__code__.co_filename, modname, themodule)</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 198</span><span class=\"ansiyellow\">             </span>self<span class=\"ansiyellow\">.</span>save_function_tuple<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    199</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    200</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function_tuple</span><span class=\"ansiblue\">(self, func)</span>\n<span class=\"ansigreen\">    244</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    245</span>         <span class=\"ansired\"># save the rest of the func data needed by _fill_function</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 246</span><span class=\"ansiyellow\">         </span>save<span class=\"ansiyellow\">(</span>f_globals<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    247</span>         save<span class=\"ansiyellow\">(</span>defaults<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    248</span>         save<span class=\"ansiyellow\">(</span>dct<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save_dict</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    653</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    654</span>         self<span class=\"ansiyellow\">.</span>memoize<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 655</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>_batch_setitems<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">.</span>iteritems<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    656</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    657</span>     dispatch<span class=\"ansiyellow\">[</span>DictionaryType<span class=\"ansiyellow\">]</span> <span class=\"ansiyellow\">=</span> save_dict<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">_batch_setitems</span><span class=\"ansiblue\">(self, items)</span>\n<span class=\"ansigreen\">    685</span>                 <span class=\"ansigreen\">for</span> k<span class=\"ansiyellow\">,</span> v <span class=\"ansigreen\">in</span> tmp<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    686</span>                     save<span class=\"ansiyellow\">(</span>k<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 687</span><span class=\"ansiyellow\">                     </span>save<span class=\"ansiyellow\">(</span>v<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    688</span>                 write<span class=\"ansiyellow\">(</span>SETITEMS<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    689</span>             <span class=\"ansigreen\">elif</span> n<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function</span><span class=\"ansiblue\">(self, obj, name)</span>\n<span class=\"ansigreen\">    196</span>         <span class=\"ansigreen\">if</span> islambda<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">or</span> obj<span class=\"ansiyellow\">.</span>__code__<span class=\"ansiyellow\">.</span>co_filename <span class=\"ansiyellow\">==</span> <span class=\"ansiblue\">&apos;&lt;stdin&gt;&apos;</span> <span class=\"ansigreen\">or</span> themodule <span class=\"ansigreen\">is</span> None<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    197</span>             <span class=\"ansired\">#print(&quot;save global&quot;, islambda(obj), obj.__code__.co_filename, modname, themodule)</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 198</span><span class=\"ansiyellow\">             </span>self<span class=\"ansiyellow\">.</span>save_function_tuple<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    199</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    200</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function_tuple</span><span class=\"ansiblue\">(self, func)</span>\n<span class=\"ansigreen\">    244</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    245</span>         <span class=\"ansired\"># save the rest of the func data needed by _fill_function</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 246</span><span class=\"ansiyellow\">         </span>save<span class=\"ansiyellow\">(</span>f_globals<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    247</span>         save<span class=\"ansiyellow\">(</span>defaults<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    248</span>         save<span class=\"ansiyellow\">(</span>dct<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save_dict</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    653</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    654</span>         self<span class=\"ansiyellow\">.</span>memoize<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 655</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>_batch_setitems<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">.</span>iteritems<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    656</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    657</span>     dispatch<span class=\"ansiyellow\">[</span>DictionaryType<span class=\"ansiyellow\">]</span> <span class=\"ansiyellow\">=</span> save_dict<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">_batch_setitems</span><span class=\"ansiblue\">(self, items)</span>\n<span class=\"ansigreen\">    685</span>                 <span class=\"ansigreen\">for</span> k<span class=\"ansiyellow\">,</span> v <span class=\"ansigreen\">in</span> tmp<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    686</span>                     save<span class=\"ansiyellow\">(</span>k<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 687</span><span class=\"ansiyellow\">                     </span>save<span class=\"ansiyellow\">(</span>v<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    688</span>                 write<span class=\"ansiyellow\">(</span>SETITEMS<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    689</span>             <span class=\"ansigreen\">elif</span> n<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function</span><span class=\"ansiblue\">(self, obj, name)</span>\n<span class=\"ansigreen\">    196</span>         <span class=\"ansigreen\">if</span> islambda<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">or</span> obj<span class=\"ansiyellow\">.</span>__code__<span class=\"ansiyellow\">.</span>co_filename <span class=\"ansiyellow\">==</span> <span class=\"ansiblue\">&apos;&lt;stdin&gt;&apos;</span> <span class=\"ansigreen\">or</span> themodule <span class=\"ansigreen\">is</span> None<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    197</span>             <span class=\"ansired\">#print(&quot;save global&quot;, islambda(obj), obj.__code__.co_filename, modname, themodule)</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 198</span><span class=\"ansiyellow\">             </span>self<span class=\"ansiyellow\">.</span>save_function_tuple<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    199</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    200</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function_tuple</span><span class=\"ansiblue\">(self, func)</span>\n<span class=\"ansigreen\">    244</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    245</span>         <span class=\"ansired\"># save the rest of the func data needed by _fill_function</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 246</span><span class=\"ansiyellow\">         </span>save<span class=\"ansiyellow\">(</span>f_globals<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    247</span>         save<span class=\"ansiyellow\">(</span>defaults<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    248</span>         save<span class=\"ansiyellow\">(</span>dct<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save_dict</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    653</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    654</span>         self<span class=\"ansiyellow\">.</span>memoize<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 655</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>_batch_setitems<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">.</span>iteritems<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    656</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    657</span>     dispatch<span class=\"ansiyellow\">[</span>DictionaryType<span class=\"ansiyellow\">]</span> <span class=\"ansiyellow\">=</span> save_dict<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">_batch_setitems</span><span class=\"ansiblue\">(self, items)</span>\n<span class=\"ansigreen\">    690</span>                 k<span class=\"ansiyellow\">,</span> v <span class=\"ansiyellow\">=</span> tmp<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    691</span>                 save<span class=\"ansiyellow\">(</span>k<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 692</span><span class=\"ansiyellow\">                 </span>save<span class=\"ansiyellow\">(</span>v<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    693</span>                 write<span class=\"ansiyellow\">(</span>SETITEM<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    694</span>             <span class=\"ansired\"># else tmp is empty, and we&apos;re done</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function</span><span class=\"ansiblue\">(self, obj, name)</span>\n<span class=\"ansigreen\">    196</span>         <span class=\"ansigreen\">if</span> islambda<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">or</span> obj<span class=\"ansiyellow\">.</span>__code__<span class=\"ansiyellow\">.</span>co_filename <span class=\"ansiyellow\">==</span> <span class=\"ansiblue\">&apos;&lt;stdin&gt;&apos;</span> <span class=\"ansigreen\">or</span> themodule <span class=\"ansigreen\">is</span> None<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    197</span>             <span class=\"ansired\">#print(&quot;save global&quot;, islambda(obj), obj.__code__.co_filename, modname, themodule)</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 198</span><span class=\"ansiyellow\">             </span>self<span class=\"ansiyellow\">.</span>save_function_tuple<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    199</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    200</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function_tuple</span><span class=\"ansiblue\">(self, func)</span>\n<span class=\"ansigreen\">    244</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    245</span>         <span class=\"ansired\"># save the rest of the func data needed by _fill_function</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 246</span><span class=\"ansiyellow\">         </span>save<span class=\"ansiyellow\">(</span>f_globals<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    247</span>         save<span class=\"ansiyellow\">(</span>defaults<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    248</span>         save<span class=\"ansiyellow\">(</span>dct<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save_dict</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    653</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    654</span>         self<span class=\"ansiyellow\">.</span>memoize<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 655</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>_batch_setitems<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">.</span>iteritems<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    656</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    657</span>     dispatch<span class=\"ansiyellow\">[</span>DictionaryType<span class=\"ansiyellow\">]</span> <span class=\"ansiyellow\">=</span> save_dict<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">_batch_setitems</span><span class=\"ansiblue\">(self, items)</span>\n<span class=\"ansigreen\">    690</span>                 k<span class=\"ansiyellow\">,</span> v <span class=\"ansiyellow\">=</span> tmp<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    691</span>                 save<span class=\"ansiyellow\">(</span>k<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 692</span><span class=\"ansiyellow\">                 </span>save<span class=\"ansiyellow\">(</span>v<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    693</span>                 write<span class=\"ansiyellow\">(</span>SETITEM<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    694</span>             <span class=\"ansired\"># else tmp is empty, and we&apos;re done</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    304</span>             reduce <span class=\"ansiyellow\">=</span> getattr<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;__reduce_ex__&quot;</span><span class=\"ansiyellow\">,</span> None<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    305</span>             <span class=\"ansigreen\">if</span> reduce<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 306</span><span class=\"ansiyellow\">                 </span>rv <span class=\"ansiyellow\">=</span> reduce<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>proto<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    307</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    308</span>                 reduce <span class=\"ansiyellow\">=</span> getattr<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;__reduce__&quot;</span><span class=\"ansiyellow\">,</span> None<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansicyan\">__getnewargs__</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    264</span>         <span class=\"ansired\"># This method is called when attempting to pickle SparkContext, which is always an error:</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    265</span>         raise Exception(\n<span class=\"ansigreen\">--&gt; 266</span><span class=\"ansiyellow\">             </span><span class=\"ansiblue\">&quot;It appears that you are attempting to reference SparkContext from a broadcast &quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    267</span>             <span class=\"ansiblue\">&quot;variable, action, or transformation. SparkContext can only be used on the driver, &quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    268</span>             <span class=\"ansiblue\">&quot;not in code that it run on workers. For more information, see SPARK-5063.&quot;</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">Exception</span>: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n</div>","workflows":[],"startTime":1.482484043211E12,"submitTime":1.482484043211E12,"finishTime":1.482484043958E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"8a8bce3c-b598-4655-a9d4-d6232e1815a4"},{"version":"CommandV1","origId":2216104459508924,"guid":"bab8d9d8-fc1b-42ec-8734-fca0e1c8fd0b","subtype":"command","commandType":"auto","position":53.0,"command":"# TEST Perform Entity Resolution (3c)\nTest.assertTrue(abs(similarityAmazonGoogle - 0.000303171940451) < 0.0000001,\n                'incorrect similarityAmazonGoogle')","commandVersion":1,"state":"error","results":null,"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482178441E12,"submitTime":1.482482178441E12,"finishTime":1.482482192353E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d8eb9057-81bd-4433-8dbb-e29abd488561"},{"version":"CommandV1","origId":2216104459508925,"guid":"9a7ede67-9476-4cc7-9f3a-497386af5a62","subtype":"command","commandType":"auto","position":54.0,"command":"%md\n#### **(3d) Perform Entity Resolution with Broadcast Variables**\nThe solution in (3c) works well for small datasets, but it requires Spark to (automatically) send the `idfsSmallWeights` variable to all the workers. If we didn't `cache()` similarities, then it might have to be recreated if we run `similar()` multiple times. This would cause Spark to send `idfsSmallWeights` every time.\n \nInstead, we can use a broadcast variable - we define the broadcast variable in the driver and then we can refer to it in each worker. Spark saves the broadcast variable at each worker, so it is only sent once.\n \nThe steps you should perform are:\n* Define a `computeSimilarityBroadcast` function that given an element from the combination RDD computes the cosine simlarity for the two records in the element. This will be the same as the worker function `computeSimilarity` in (3c) except that it uses a broadcast variable.\n* Apply the worker function to every element in the RDD\n \nAgain, compute the similarity between Amazon record `b000o24l3q` and Google record `http://www.google.com/base/feeds/snippets/17242822440574356561`.","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482178452E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"cc2d3d66-ea64-4a1f-8ac6-d3a002465e2f"},{"version":"CommandV1","origId":2216104459508926,"guid":"c4472d5f-03ba-4ac9-8f88-89cc092a7523","subtype":"command","commandType":"auto","position":55.0,"command":"# TODO: Replace <FILL IN> with appropriate code\ndef computeSimilarityBroadcast(record):\n    \"\"\" Compute similarity on a combination record, using Broadcast variable\n    Args:\n        record: a pair, (google record, amazon record)\n    Returns:\n        pair: a pair, (google URL, amazon ID, cosine similarity value)\n    \"\"\"\n    googleRec = record[0]\n    amazonRec = record[1]\n    googleURL = googleRec[0]\n    amazonID = amazonRec[0]\n    googleValue = googleRec[1]\n    amazonValue = amazonRec[1]\n    cs = cosineSimilarity(googleValue, amazonValue, idfsSmallBroadcast.value)\n    return (googleURL, amazonID, cs)\n\nidfsSmallBroadcast = sc.broadcast(idfsSmallWeights)\nsimilaritiesBroadcast = (crossSmall\n                         .map(computeSimilarity)\n                         .cache())\n\ndef similarBroadcast(amazonID, googleURL):\n    \"\"\" Return similarity value, computed using Broadcast variable\n    Args:\n        amazonID: amazon ID\n        googleURL: google URL\n    Returns:\n        similar: cosine similarity value\n    \"\"\"\n    return (similaritiesBroadcast\n            .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))\n            .collect()[0][2])\n\nsimilarityAmazonGoogleBroadcast = similarBroadcast('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')\nprint 'Requested similarity is %s.' % similarityAmazonGoogleBroadcast","commandVersion":1,"state":"error","results":null,"errorSummary":"<span class=\"ansired\">Exception</span>: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Exception</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-88-32d6f000957f&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     18</span> idfsSmallBroadcast <span class=\"ansiyellow\">=</span> sc<span class=\"ansiyellow\">.</span>broadcast<span class=\"ansiyellow\">(</span>idfsSmallWeights<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     19</span> similaritiesBroadcast = (crossSmall\n<span class=\"ansigreen\">---&gt; 20</span><span class=\"ansiyellow\">                          </span><span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span>computeSimilarity<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     21</span>                          .cache())\n<span class=\"ansigreen\">     22</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">cache</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    224</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">    225</span>         self<span class=\"ansiyellow\">.</span>is_cached <span class=\"ansiyellow\">=</span> True<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 226</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>persist<span class=\"ansiyellow\">(</span>StorageLevel<span class=\"ansiyellow\">.</span>MEMORY_ONLY<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    227</span>         <span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    228</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">persist</span><span class=\"ansiblue\">(self, storageLevel)</span>\n<span class=\"ansigreen\">    240</span>         self<span class=\"ansiyellow\">.</span>is_cached <span class=\"ansiyellow\">=</span> True<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    241</span>         javaStorageLevel <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>ctx<span class=\"ansiyellow\">.</span>_getJavaStorageLevel<span class=\"ansiyellow\">(</span>storageLevel<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 242</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>_jrdd<span class=\"ansiyellow\">.</span>persist<span class=\"ansiyellow\">(</span>javaStorageLevel<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    243</span>         <span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    244</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">_jrdd</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">   2401</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2402</span>         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n<span class=\"ansigreen\">-&gt; 2403</span><span class=\"ansiyellow\">                                       self._jrdd_deserializer, profiler)\n</span><span class=\"ansigreen\">   2404</span>         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n<span class=\"ansigreen\">   2405</span>                                              self.preservesPartitioning)\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">_wrap_function</span><span class=\"ansiblue\">(sc, func, deserializer, serializer, profiler)</span>\n<span class=\"ansigreen\">   2334</span>     <span class=\"ansigreen\">assert</span> serializer<span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;serializer should not be empty&quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2335</span>     command <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">(</span>func<span class=\"ansiyellow\">,</span> profiler<span class=\"ansiyellow\">,</span> deserializer<span class=\"ansiyellow\">,</span> serializer<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 2336</span><span class=\"ansiyellow\">     </span>pickled_command<span class=\"ansiyellow\">,</span> broadcast_vars<span class=\"ansiyellow\">,</span> env<span class=\"ansiyellow\">,</span> includes <span class=\"ansiyellow\">=</span> _prepare_for_python_RDD<span class=\"ansiyellow\">(</span>sc<span class=\"ansiyellow\">,</span> command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2337</span>     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n<span class=\"ansigreen\">   2338</span>                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansicyan\">_prepare_for_python_RDD</span><span class=\"ansiblue\">(sc, command)</span>\n<span class=\"ansigreen\">   2313</span>     <span class=\"ansired\"># the serialized command will be compressed by broadcast</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2314</span>     ser <span class=\"ansiyellow\">=</span> CloudPickleSerializer<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 2315</span><span class=\"ansiyellow\">     </span>pickled_command <span class=\"ansiyellow\">=</span> ser<span class=\"ansiyellow\">.</span>dumps<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2316</span>     <span class=\"ansigreen\">if</span> len<span class=\"ansiyellow\">(</span>pickled_command<span class=\"ansiyellow\">)</span> <span class=\"ansiyellow\">&gt;</span> <span class=\"ansiyellow\">(</span><span class=\"ansicyan\">1</span> <span class=\"ansiyellow\">&lt;&lt;</span> <span class=\"ansicyan\">20</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span>  <span class=\"ansired\"># 1M</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2317</span>         <span class=\"ansired\"># The broadcast will have same life cycle as created PythonRDD</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/serializers.py</span> in <span class=\"ansicyan\">dumps</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    426</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    427</span>     <span class=\"ansigreen\">def</span> dumps<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 428</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> cloudpickle<span class=\"ansiyellow\">.</span>dumps<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">2</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    429</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    430</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">dumps</span><span class=\"ansiblue\">(obj, protocol)</span>\n<span class=\"ansigreen\">    655</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    656</span>     cp <span class=\"ansiyellow\">=</span> CloudPickler<span class=\"ansiyellow\">(</span>file<span class=\"ansiyellow\">,</span>protocol<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 657</span><span class=\"ansiyellow\">     </span>cp<span class=\"ansiyellow\">.</span>dump<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    658</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    659</span>     <span class=\"ansigreen\">return</span> file<span class=\"ansiyellow\">.</span>getvalue<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">dump</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    105</span>         self<span class=\"ansiyellow\">.</span>inject_addons<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    106</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 107</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> Pickler<span class=\"ansiyellow\">.</span>dump<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    108</span>         <span class=\"ansigreen\">except</span> RuntimeError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    109</span>             <span class=\"ansigreen\">if</span> <span class=\"ansiblue\">&apos;recursion&apos;</span> <span class=\"ansigreen\">in</span> e<span class=\"ansiyellow\">.</span>args<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">dump</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    222</span>         <span class=\"ansigreen\">if</span> self<span class=\"ansiyellow\">.</span>proto <span class=\"ansiyellow\">&gt;=</span> <span class=\"ansicyan\">2</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    223</span>             self<span class=\"ansiyellow\">.</span>write<span class=\"ansiyellow\">(</span>PROTO <span class=\"ansiyellow\">+</span> chr<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>proto<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 224</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>save<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    225</span>         self<span class=\"ansiyellow\">.</span>write<span class=\"ansiyellow\">(</span>STOP<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    226</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save_tuple</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    566</span>         write<span class=\"ansiyellow\">(</span>MARK<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    567</span>         <span class=\"ansigreen\">for</span> element <span class=\"ansigreen\">in</span> obj<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 568</span><span class=\"ansiyellow\">             </span>save<span class=\"ansiyellow\">(</span>element<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    569</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    570</span>         <span class=\"ansigreen\">if</span> id<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">in</span> memo<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function</span><span class=\"ansiblue\">(self, obj, name)</span>\n<span class=\"ansigreen\">    202</span>             klass <span class=\"ansiyellow\">=</span> getattr<span class=\"ansiyellow\">(</span>themodule<span class=\"ansiyellow\">,</span> name<span class=\"ansiyellow\">,</span> None<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    203</span>             <span class=\"ansigreen\">if</span> klass <span class=\"ansigreen\">is</span> None <span class=\"ansigreen\">or</span> klass <span class=\"ansigreen\">is</span> <span class=\"ansigreen\">not</span> obj<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 204</span><span class=\"ansiyellow\">                 </span>self<span class=\"ansiyellow\">.</span>save_function_tuple<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    205</span>                 <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    206</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function_tuple</span><span class=\"ansiblue\">(self, func)</span>\n<span class=\"ansigreen\">    239</span>         <span class=\"ansired\"># create a skeleton function object and memoize it</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    240</span>         save<span class=\"ansiyellow\">(</span>_make_skel_func<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 241</span><span class=\"ansiyellow\">         </span>save<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">(</span>code<span class=\"ansiyellow\">,</span> closure<span class=\"ansiyellow\">,</span> base_globals<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    242</span>         write<span class=\"ansiyellow\">(</span>pickle<span class=\"ansiyellow\">.</span>REDUCE<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    243</span>         self<span class=\"ansiyellow\">.</span>memoize<span class=\"ansiyellow\">(</span>func<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save_tuple</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    552</span>         <span class=\"ansigreen\">if</span> n <span class=\"ansiyellow\">&lt;=</span> <span class=\"ansicyan\">3</span> <span class=\"ansigreen\">and</span> proto <span class=\"ansiyellow\">&gt;=</span> <span class=\"ansicyan\">2</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    553</span>             <span class=\"ansigreen\">for</span> element <span class=\"ansigreen\">in</span> obj<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 554</span><span class=\"ansiyellow\">                 </span>save<span class=\"ansiyellow\">(</span>element<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    555</span>             <span class=\"ansired\"># Subtle.  Same as in the big comment below.</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    556</span>             <span class=\"ansigreen\">if</span> id<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">in</span> memo<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save_list</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    604</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    605</span>         self<span class=\"ansiyellow\">.</span>memoize<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 606</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>_batch_appends<span class=\"ansiyellow\">(</span>iter<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    607</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    608</span>     dispatch<span class=\"ansiyellow\">[</span>ListType<span class=\"ansiyellow\">]</span> <span class=\"ansiyellow\">=</span> save_list<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">_batch_appends</span><span class=\"ansiblue\">(self, items)</span>\n<span class=\"ansigreen\">    640</span>                 write<span class=\"ansiyellow\">(</span>APPENDS<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    641</span>             <span class=\"ansigreen\">elif</span> n<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 642</span><span class=\"ansiyellow\">                 </span>save<span class=\"ansiyellow\">(</span>tmp<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    643</span>                 write<span class=\"ansiyellow\">(</span>APPEND<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    644</span>             <span class=\"ansired\"># else tmp is empty, and we&apos;re done</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function</span><span class=\"ansiblue\">(self, obj, name)</span>\n<span class=\"ansigreen\">    196</span>         <span class=\"ansigreen\">if</span> islambda<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">or</span> obj<span class=\"ansiyellow\">.</span>__code__<span class=\"ansiyellow\">.</span>co_filename <span class=\"ansiyellow\">==</span> <span class=\"ansiblue\">&apos;&lt;stdin&gt;&apos;</span> <span class=\"ansigreen\">or</span> themodule <span class=\"ansigreen\">is</span> None<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    197</span>             <span class=\"ansired\">#print(&quot;save global&quot;, islambda(obj), obj.__code__.co_filename, modname, themodule)</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 198</span><span class=\"ansiyellow\">             </span>self<span class=\"ansiyellow\">.</span>save_function_tuple<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    199</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    200</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function_tuple</span><span class=\"ansiblue\">(self, func)</span>\n<span class=\"ansigreen\">    244</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    245</span>         <span class=\"ansired\"># save the rest of the func data needed by _fill_function</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 246</span><span class=\"ansiyellow\">         </span>save<span class=\"ansiyellow\">(</span>f_globals<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    247</span>         save<span class=\"ansiyellow\">(</span>defaults<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    248</span>         save<span class=\"ansiyellow\">(</span>dct<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save_dict</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    653</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    654</span>         self<span class=\"ansiyellow\">.</span>memoize<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 655</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>_batch_setitems<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">.</span>iteritems<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    656</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    657</span>     dispatch<span class=\"ansiyellow\">[</span>DictionaryType<span class=\"ansiyellow\">]</span> <span class=\"ansiyellow\">=</span> save_dict<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">_batch_setitems</span><span class=\"ansiblue\">(self, items)</span>\n<span class=\"ansigreen\">    685</span>                 <span class=\"ansigreen\">for</span> k<span class=\"ansiyellow\">,</span> v <span class=\"ansigreen\">in</span> tmp<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    686</span>                     save<span class=\"ansiyellow\">(</span>k<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 687</span><span class=\"ansiyellow\">                     </span>save<span class=\"ansiyellow\">(</span>v<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    688</span>                 write<span class=\"ansiyellow\">(</span>SETITEMS<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    689</span>             <span class=\"ansigreen\">elif</span> n<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function</span><span class=\"ansiblue\">(self, obj, name)</span>\n<span class=\"ansigreen\">    196</span>         <span class=\"ansigreen\">if</span> islambda<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">or</span> obj<span class=\"ansiyellow\">.</span>__code__<span class=\"ansiyellow\">.</span>co_filename <span class=\"ansiyellow\">==</span> <span class=\"ansiblue\">&apos;&lt;stdin&gt;&apos;</span> <span class=\"ansigreen\">or</span> themodule <span class=\"ansigreen\">is</span> None<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    197</span>             <span class=\"ansired\">#print(&quot;save global&quot;, islambda(obj), obj.__code__.co_filename, modname, themodule)</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 198</span><span class=\"ansiyellow\">             </span>self<span class=\"ansiyellow\">.</span>save_function_tuple<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    199</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    200</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function_tuple</span><span class=\"ansiblue\">(self, func)</span>\n<span class=\"ansigreen\">    244</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    245</span>         <span class=\"ansired\"># save the rest of the func data needed by _fill_function</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 246</span><span class=\"ansiyellow\">         </span>save<span class=\"ansiyellow\">(</span>f_globals<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    247</span>         save<span class=\"ansiyellow\">(</span>defaults<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    248</span>         save<span class=\"ansiyellow\">(</span>dct<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save_dict</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    653</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    654</span>         self<span class=\"ansiyellow\">.</span>memoize<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 655</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>_batch_setitems<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">.</span>iteritems<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    656</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    657</span>     dispatch<span class=\"ansiyellow\">[</span>DictionaryType<span class=\"ansiyellow\">]</span> <span class=\"ansiyellow\">=</span> save_dict<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">_batch_setitems</span><span class=\"ansiblue\">(self, items)</span>\n<span class=\"ansigreen\">    685</span>                 <span class=\"ansigreen\">for</span> k<span class=\"ansiyellow\">,</span> v <span class=\"ansigreen\">in</span> tmp<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    686</span>                     save<span class=\"ansiyellow\">(</span>k<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 687</span><span class=\"ansiyellow\">                     </span>save<span class=\"ansiyellow\">(</span>v<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    688</span>                 write<span class=\"ansiyellow\">(</span>SETITEMS<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    689</span>             <span class=\"ansigreen\">elif</span> n<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function</span><span class=\"ansiblue\">(self, obj, name)</span>\n<span class=\"ansigreen\">    196</span>         <span class=\"ansigreen\">if</span> islambda<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">or</span> obj<span class=\"ansiyellow\">.</span>__code__<span class=\"ansiyellow\">.</span>co_filename <span class=\"ansiyellow\">==</span> <span class=\"ansiblue\">&apos;&lt;stdin&gt;&apos;</span> <span class=\"ansigreen\">or</span> themodule <span class=\"ansigreen\">is</span> None<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    197</span>             <span class=\"ansired\">#print(&quot;save global&quot;, islambda(obj), obj.__code__.co_filename, modname, themodule)</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 198</span><span class=\"ansiyellow\">             </span>self<span class=\"ansiyellow\">.</span>save_function_tuple<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    199</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    200</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function_tuple</span><span class=\"ansiblue\">(self, func)</span>\n<span class=\"ansigreen\">    244</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    245</span>         <span class=\"ansired\"># save the rest of the func data needed by _fill_function</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 246</span><span class=\"ansiyellow\">         </span>save<span class=\"ansiyellow\">(</span>f_globals<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    247</span>         save<span class=\"ansiyellow\">(</span>defaults<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    248</span>         save<span class=\"ansiyellow\">(</span>dct<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save_dict</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    653</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    654</span>         self<span class=\"ansiyellow\">.</span>memoize<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 655</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>_batch_setitems<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">.</span>iteritems<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    656</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    657</span>     dispatch<span class=\"ansiyellow\">[</span>DictionaryType<span class=\"ansiyellow\">]</span> <span class=\"ansiyellow\">=</span> save_dict<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">_batch_setitems</span><span class=\"ansiblue\">(self, items)</span>\n<span class=\"ansigreen\">    690</span>                 k<span class=\"ansiyellow\">,</span> v <span class=\"ansiyellow\">=</span> tmp<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    691</span>                 save<span class=\"ansiyellow\">(</span>k<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 692</span><span class=\"ansiyellow\">                 </span>save<span class=\"ansiyellow\">(</span>v<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    693</span>                 write<span class=\"ansiyellow\">(</span>SETITEM<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    694</span>             <span class=\"ansired\"># else tmp is empty, and we&apos;re done</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function</span><span class=\"ansiblue\">(self, obj, name)</span>\n<span class=\"ansigreen\">    196</span>         <span class=\"ansigreen\">if</span> islambda<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">or</span> obj<span class=\"ansiyellow\">.</span>__code__<span class=\"ansiyellow\">.</span>co_filename <span class=\"ansiyellow\">==</span> <span class=\"ansiblue\">&apos;&lt;stdin&gt;&apos;</span> <span class=\"ansigreen\">or</span> themodule <span class=\"ansigreen\">is</span> None<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    197</span>             <span class=\"ansired\">#print(&quot;save global&quot;, islambda(obj), obj.__code__.co_filename, modname, themodule)</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 198</span><span class=\"ansiyellow\">             </span>self<span class=\"ansiyellow\">.</span>save_function_tuple<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    199</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    200</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/cloudpickle.py</span> in <span class=\"ansicyan\">save_function_tuple</span><span class=\"ansiblue\">(self, func)</span>\n<span class=\"ansigreen\">    244</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    245</span>         <span class=\"ansired\"># save the rest of the func data needed by _fill_function</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 246</span><span class=\"ansiyellow\">         </span>save<span class=\"ansiyellow\">(</span>f_globals<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    247</span>         save<span class=\"ansiyellow\">(</span>defaults<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    248</span>         save<span class=\"ansiyellow\">(</span>dct<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    284</span>         f <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>dispatch<span class=\"ansiyellow\">.</span>get<span class=\"ansiyellow\">(</span>t<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    285</span>         <span class=\"ansigreen\">if</span> f<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 286</span><span class=\"ansiyellow\">             </span>f<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> obj<span class=\"ansiyellow\">)</span> <span class=\"ansired\"># Call unbound method with explicit self</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>             <span class=\"ansigreen\">return</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    288</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save_dict</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    653</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    654</span>         self<span class=\"ansiyellow\">.</span>memoize<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 655</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>_batch_setitems<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">.</span>iteritems<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    656</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    657</span>     dispatch<span class=\"ansiyellow\">[</span>DictionaryType<span class=\"ansiyellow\">]</span> <span class=\"ansiyellow\">=</span> save_dict<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">_batch_setitems</span><span class=\"ansiblue\">(self, items)</span>\n<span class=\"ansigreen\">    690</span>                 k<span class=\"ansiyellow\">,</span> v <span class=\"ansiyellow\">=</span> tmp<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    691</span>                 save<span class=\"ansiyellow\">(</span>k<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 692</span><span class=\"ansiyellow\">                 </span>save<span class=\"ansiyellow\">(</span>v<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    693</span>                 write<span class=\"ansiyellow\">(</span>SETITEM<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    694</span>             <span class=\"ansired\"># else tmp is empty, and we&apos;re done</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/usr/lib/python2.7/pickle.pyc</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, obj)</span>\n<span class=\"ansigreen\">    304</span>             reduce <span class=\"ansiyellow\">=</span> getattr<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;__reduce_ex__&quot;</span><span class=\"ansiyellow\">,</span> None<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    305</span>             <span class=\"ansigreen\">if</span> reduce<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 306</span><span class=\"ansiyellow\">                 </span>rv <span class=\"ansiyellow\">=</span> reduce<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>proto<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    307</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    308</span>                 reduce <span class=\"ansiyellow\">=</span> getattr<span class=\"ansiyellow\">(</span>obj<span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;__reduce__&quot;</span><span class=\"ansiyellow\">,</span> None<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansicyan\">__getnewargs__</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    264</span>         <span class=\"ansired\"># This method is called when attempting to pickle SparkContext, which is always an error:</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    265</span>         raise Exception(\n<span class=\"ansigreen\">--&gt; 266</span><span class=\"ansiyellow\">             </span><span class=\"ansiblue\">&quot;It appears that you are attempting to reference SparkContext from a broadcast &quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    267</span>             <span class=\"ansiblue\">&quot;variable, action, or transformation. SparkContext can only be used on the driver, &quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    268</span>             <span class=\"ansiblue\">&quot;not in code that it run on workers. For more information, see SPARK-5063.&quot;</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">Exception</span>: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n</div>","workflows":[],"startTime":1.482484165086E12,"submitTime":1.482484165086E12,"finishTime":1.482484165842E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f02583fe-64f9-4955-8b9c-9fa0076ea70c"},{"version":"CommandV1","origId":2216104459508927,"guid":"be7c67e5-a09f-403b-818d-5b9d581ef35d","subtype":"command","commandType":"auto","position":56.0,"command":"# TEST Perform Entity Resolution with Broadcast Variables (3d)\nfrom pyspark import Broadcast\nTest.assertTrue(isinstance(idfsSmallBroadcast, Broadcast), 'incorrect idfsSmallBroadcast')\nTest.assertEquals(len(idfsSmallBroadcast.value), 4772, 'incorrect idfsSmallBroadcast value')\nTest.assertTrue(abs(similarityAmazonGoogleBroadcast - 0.000303171940451) < 0.0000001,\n                'incorrect similarityAmazonGoogle')","commandVersion":1,"state":"error","results":null,"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.48248217848E12,"submitTime":1.48248217848E12,"finishTime":1.482482192355E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"5d97e8d3-96f4-4624-b5bd-27167464c60a"},{"version":"CommandV1","origId":2216104459508928,"guid":"37134809-bdda-44a1-bede-a62c09ad4b8e","subtype":"command","commandType":"auto","position":57.0,"command":"%md\n#### **(3e) Perform a Gold Standard evaluation**\n \nFirst, we'll load the \"gold standard\" data and use it to answer several questions. We read and parse the Gold Standard data, where the format of each line is \"Amazon Product ID\",\"Google URL\". The resulting RDD has elements of the form (\"AmazonID GoogleURL\", 'gold')","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482178491E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ad7e0216-abda-416e-907b-770fb02ccda5"},{"version":"CommandV1","origId":2216104459508929,"guid":"3f096cb5-d86e-4074-a605-39066ca3c23d","subtype":"command","commandType":"auto","position":58.0,"command":"GOLDFILE_PATTERN = '^(.+),(.+)'\n\n# Parse each line of a data file useing the specified regular expression pattern\ndef parse_goldfile_line(goldfile_line):\n    \"\"\" Parse a line from the 'golden standard' data file\n    Args:\n        goldfile_line: a line of data\n    Returns:\n        pair: ((key, 'gold', 1 if successful or else 0))\n    \"\"\"\n    match = re.search(GOLDFILE_PATTERN, goldfile_line)\n    if match is None:\n        print 'Invalid goldfile line: %s' % goldfile_line\n        return (goldfile_line, -1)\n    elif match.group(1) == '\"idAmazon\"':\n        print 'Header datafile line: %s' % goldfile_line\n        return (goldfile_line, 0)\n    else:\n        key = '%s %s' % (removeQuotes(match.group(1)), removeQuotes(match.group(2)))\n        return ((key, 'gold'), 1)\n\ngoldfile = os.path.join(baseDir, inputPath, GOLD_STANDARD_PATH)\ngsRaw = (sc\n         .textFile(goldfile)\n         .map(parse_goldfile_line)\n         .cache())\n\ngsFailed = (gsRaw\n            .filter(lambda s: s[1] == -1)\n            .map(lambda s: s[0]))\nfor line in gsFailed.take(10):\n    print 'Invalid goldfile line: %s' % line\n\ngoldStandard = (gsRaw\n                .filter(lambda s: s[1] == 1)\n                .map(lambda s: s[0])\n                .cache())\n\nprint 'Read %d lines, successfully parsed %d lines, failed to parse %d lines' % (gsRaw.count(),\n                                                                                 goldStandard.count(),\n                                                                                 gsFailed.count())\nassert (gsFailed.count() == 0)\nassert (gsRaw.count() == (goldStandard.count() + 1))","commandVersion":1,"state":"error","results":null,"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482178508E12,"submitTime":1.482482178508E12,"finishTime":1.482482192355E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9a203dc7-fbd0-4197-bed1-18d5d34e5e47"},{"version":"CommandV1","origId":2216104459508930,"guid":"d064a136-004b-4250-b306-46dbaeef9a56","subtype":"command","commandType":"auto","position":59.0,"command":"%md\n#### Using the \"gold standard\" data we can answer the following questions:\n \n* How many true duplicate pairs are there in the small datasets?\n* What is the average similarity score for true duplicates?\n* What about for non-duplicates?\nThe steps you should perform are:\n* Create a new `sims` RDD from the `similaritiesBroadcast` RDD, where each element consists of a pair of the form (\"AmazonID GoogleURL\", cosineSimilarityScore). An example entry from `sims` is: ('b000bi7uqs http://www.google.com/base/feeds/snippets/18403148885652932189', 0.40202896125621296)\n* Combine the `sims` RDD with the `goldStandard` RDD by creating a new `trueDupsRDD` RDD that has the just the cosine similarity scores for those \"AmazonID GoogleURL\" pairs that appear in both the `sims` RDD and `goldStandard` RDD. Hint: you can do this using the join() transformation.\n* Count the number of true duplicate pairs in the `trueDupsRDD` dataset\n* Compute the average similarity score for true duplicates in the `trueDupsRDD` datasets. Remember to use `float` for calculation\n* Create a new `nonDupsRDD` RDD that has the just the cosine similarity scores for those \"AmazonID GoogleURL\" pairs from the `similaritiesBroadcast` RDD that **do not** appear in both the *sims* RDD and gold standard RDD.\n* Compute the average similarity score for non-duplicates in the last datasets. Remember to use `float` for calculation","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482178519E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"8aadb65d-fd2b-4dc1-9512-57bc5b9b992d"},{"version":"CommandV1","origId":2216104459508931,"guid":"929282f9-bb3a-4b9c-8073-65141780fd94","subtype":"command","commandType":"auto","position":60.0,"command":"# TODO: Replace <FILL IN> with appropriate code\nsims = similaritiesBroadcast.<FILL IN>)\n\ntrueDupsRDD = (sims\n               .<FILL IN>)\ntrueDupsCount = trueDupsRDD.<FILL IN>\navgSimDups = <FILL IN>\n\nnonDupsRDD = (sims\n              .<FILL IN>)\navgSimNon = <FILL IN>\n\nprint 'There are %s true duplicates.' % trueDupsCount\nprint 'The average similarity of true duplicates is %s.' % avgSimDups\nprint 'And for non duplicates, it is %s.' % avgSimNon","commandVersion":1,"state":"error","results":null,"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482178538E12,"submitTime":1.482482178538E12,"finishTime":1.482482192356E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f09611cc-d4c9-4f20-abee-b29f50b09ec9"},{"version":"CommandV1","origId":2216104459508932,"guid":"9a7bb32a-344a-4baf-aaef-7d07566bda0e","subtype":"command","commandType":"auto","position":61.0,"command":"# TEST Perform a Gold Standard evaluation (3e)\nTest.assertEquals(trueDupsCount, 146, 'incorrect trueDupsCount')\nTest.assertTrue(abs(avgSimDups - 0.264332573435) < 0.0000001, 'incorrect avgSimDups')\nTest.assertTrue(abs(avgSimNon - 0.00123476304656) < 0.0000001, 'incorrect avgSimNon')","commandVersion":1,"state":"error","results":null,"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482178548E12,"submitTime":1.482482178548E12,"finishTime":1.482482192357E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"fdc147ba-6df9-417b-9d95-c387d5164235"},{"version":"CommandV1","origId":2216104459508933,"guid":"da56c0b7-8e14-4d5c-ae45-9398dc030772","subtype":"command","commandType":"auto","position":62.0,"command":"%md\n#### **Part 4: Scalable ER**\nIn the previous parts, we built a text similarity function and used it for small scale entity resolution.  Our implementation is limited by its quadratic run time complexity, and is not practical for even modestly sized datasets.  In this part, we will implement a more scalable algorithm and use it to do entity resolution on the full dataset.\n \n#### Inverted Indices\nTo improve our ER algorithm from the earlier parts, we should begin by analyzing its running time. In particular, the algorithm above is quadratic in two ways. First, we did a lot of redundant computation of tokens and weights, since each record was reprocessed every time it was compared. Second, we made quadratically many token comparisons between records.\n \nThe first source of quadratic overhead can be eliminated with precomputation and look-up tables, but the second source is a little more tricky. In the worst case, every token in every record in one dataset exists in every record in the other dataset, and therefore every token makes a non-zero contribution to the cosine similarity. In this case, token comparison is unavoidably quadratic.\n \nBut in reality most records have nothing (or very little) in common. Moreover, it is typical for a record in one dataset to have at most one duplicate record in the other dataset (this is the case assuming each dataset has been de-duplicated against itself). In this case, the output is linear in the size of the input and we can hope to achieve linear running time.\n \nAn [**inverted index**](https://en.wikipedia.org/wiki/Inverted_index) is a data structure that will allow us to avoid making quadratically many token comparisons.  It maps each token in the dataset to the list of documents that contain the token.  So, instead of comparing, record by record, each token to every other token to see if they match, we will use inverted indices to *look up* records that match on a particular token.\n \n> **Note on terminology**: In text search, a *forward* index maps documents in a dataset to the tokens they contain.  An *inverted* index supports the inverse mapping.\n \n> **Note**: For this section, use the complete Google and Amazon datasets, not the samples","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482178559E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"003a8911-2dad-427d-bfe9-8c82219681c5"},{"version":"CommandV1","origId":2216104459508934,"guid":"9adebd4e-41ea-46be-9b4a-395c828aa0ee","subtype":"command","commandType":"auto","position":63.0,"command":"%md\n#### **(4a) Tokenize the full dataset**\nTokenize each of the two full datasets for Google and Amazon.","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482178579E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"3dcb32db-374a-4422-a7cd-66ffe5d0dc85"},{"version":"CommandV1","origId":2216104459508935,"guid":"85a08a02-34cf-46d9-a068-ea5430fb1f9c","subtype":"command","commandType":"auto","position":64.0,"command":"# TODO: Replace <FILL IN> with appropriate code\namazonFullRecToToken = amazon.<FILL IN>\ngoogleFullRecToToken = google.<FILL IN>\nprint 'Amazon full dataset is %s products, Google full dataset is %s products' % (amazonFullRecToToken.count(),\n                                                                                    googleFullRecToToken.count())","commandVersion":1,"state":"error","results":null,"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482178595E12,"submitTime":1.482482178595E12,"finishTime":1.482482192358E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"65e60ab9-40b7-4f6b-a82d-cfefca179e01"},{"version":"CommandV1","origId":2216104459508936,"guid":"11488083-96a5-4db3-a8d9-bc1821d4a4c0","subtype":"command","commandType":"auto","position":65.0,"command":"# TEST Tokenize the full dataset (4a)\nTest.assertEquals(amazonFullRecToToken.count(), 1363, 'incorrect amazonFullRecToToken.count()')\nTest.assertEquals(googleFullRecToToken.count(), 3226, 'incorrect googleFullRecToToken.count()')","commandVersion":1,"state":"error","results":null,"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482178606E12,"submitTime":1.482482178606E12,"finishTime":1.482482192359E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e70c0ead-d270-44e0-9923-f4e229b2c9b4"},{"version":"CommandV1","origId":2216104459508937,"guid":"01bf51cf-5839-4f57-b302-55dbdba60be4","subtype":"command","commandType":"auto","position":66.0,"command":"%md\n#### **(4b) Compute IDFs and TF-IDFs for the full datasets**\n \nWe will reuse your code from above to compute IDF weights for the complete combined datasets.\nThe steps you should perform are:\n* Create a new `fullCorpusRDD` that contains the tokens from the full Amazon and Google datasets.\n* Apply your `idfs` function to the `fullCorpusRDD`\n* Create a broadcast variable containing a dictionary of the IDF weights for the full dataset.\n* For each of the Amazon and Google full datasets, create weight RDDs that map IDs/URLs to TF-IDF weighted token vectors.","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482178617E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"95e9dac1-177d-4250-875c-a7c560d08efd"},{"version":"CommandV1","origId":2216104459508938,"guid":"a650097e-8d3b-43b5-8a61-b4ab0165a079","subtype":"command","commandType":"auto","position":67.0,"command":"# TODO: Replace <FILL IN> with appropriate code\nfullCorpusRDD = <FILL IN>\nidfsFull = idfs(fullCorpusRDD)\nidfsFullCount = idfsFull.count()\nprint 'There are %s unique tokens in the full datasets.' % idfsFullCount\n\n# Recompute IDFs for full dataset\nidfsFullWeights = <FILL IN>\nidfsFullBroadcast = <FILL IN>\n\n# Pre-compute TF-IDF weights.  Build mappings from record ID weight vector.\namazonWeightsRDD = <FILL IN>\ngoogleWeightsRDD = <FILL IN>\nprint 'There are %s Amazon weights and %s Google weights.' % (amazonWeightsRDD.count(),\n                                                              googleWeightsRDD.count())","commandVersion":1,"state":"error","results":null,"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482178633E12,"submitTime":1.482482178633E12,"finishTime":1.482482192359E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"fdb0bc7e-045d-4773-b93f-0d2fdc4254b1"},{"version":"CommandV1","origId":2216104459508939,"guid":"21bf7cc8-76c3-46a1-b1c4-2877c29c4c88","subtype":"command","commandType":"auto","position":68.0,"command":"# TEST Compute IDFs and TF-IDFs for the full datasets (4b)\nTest.assertEquals(idfsFullCount, 17078, 'incorrect idfsFullCount')\nTest.assertEquals(amazonWeightsRDD.count(), 1363, 'incorrect amazonWeightsRDD.count()')\nTest.assertEquals(googleWeightsRDD.count(), 3226, 'incorrect googleWeightsRDD.count()')","commandVersion":1,"state":"error","results":null,"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482178643E12,"submitTime":1.482482178643E12,"finishTime":1.48248219236E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"5f971504-99c5-4ba7-8e69-1e36587cbdc2"},{"version":"CommandV1","origId":2216104459508940,"guid":"9c1f74a3-faa0-490e-b494-71cf57d5042f","subtype":"command","commandType":"auto","position":69.0,"command":"%md\n#### **(4c) Compute Norms for the weights from the full datasets**\n \nWe will reuse your code from above to compute norms of the IDF weights for the complete combined dataset.\nThe steps you should perform are:\n* Create two collections, one for each of the full Amazon and Google datasets, where IDs/URLs map to the norm of the associated TF-IDF weighted token vectors.\n* Convert each collection into a broadcast variable, containing a dictionary of the norm of IDF weights for the full dataset","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482178653E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b34a5b67-bf88-4070-b601-5517fc28203d"},{"version":"CommandV1","origId":2216104459508941,"guid":"1ca47abf-e418-4f24-8b6b-8c7d64809669","subtype":"command","commandType":"auto","position":70.0,"command":"# TODO: Replace <FILL IN> with appropriate code\namazonNorms = amazonWeightsRDD.<FILL IN>\namazonNormsBroadcast = <FILL IN>\ngoogleNorms = googleWeightsRDD.<FILL IN>\ngoogleNormsBroadcast = <FILL IN>","commandVersion":1,"state":"error","results":null,"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482178669E12,"submitTime":1.482482178669E12,"finishTime":1.482482192361E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"dba68e3a-ffb1-401f-8aaa-e8026642b7cf"},{"version":"CommandV1","origId":2216104459508942,"guid":"727d6e25-de2c-4986-bac3-94dd398619d4","subtype":"command","commandType":"auto","position":71.0,"command":"# TEST Compute Norms for the weights from the full datasets (4c)\nTest.assertTrue(isinstance(amazonNormsBroadcast, Broadcast), 'incorrect amazonNormsBroadcast')\nTest.assertEquals(len(amazonNormsBroadcast.value), 1363, 'incorrect amazonNormsBroadcast.value')\nTest.assertTrue(isinstance(googleNormsBroadcast, Broadcast), 'incorrect googleNormsBroadcast')\nTest.assertEquals(len(googleNormsBroadcast.value), 3226, 'incorrect googleNormsBroadcast.value')","commandVersion":1,"state":"error","results":null,"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482178679E12,"submitTime":1.482482178679E12,"finishTime":1.482482192362E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"86a3abee-e112-48ab-88e9-93b2b9bed1a5"},{"version":"CommandV1","origId":2216104459508943,"guid":"08ecc57d-7bd0-4431-a855-1338c57288a4","subtype":"command","commandType":"auto","position":72.0,"command":"%md\n#### **(4d) Create inverted indicies from the full datasets**\n \nBuild inverted indices of both data sources.\nThe steps you should perform are:\n* Create an invert function that given a pair of (ID/URL, TF-IDF weighted token vector), returns a list of pairs of (token, ID/URL). Recall that the TF-IDF weighted token vector is a Python dictionary with keys that are tokens and values that are weights.\n* Use your invert function to convert the full Amazon and Google TF-IDF weighted token vector datasets into two RDDs where each element is a pair of a token and an ID/URL that contain that token. These are inverted indicies.","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.48248217869E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"2a917898-05be-458b-b4aa-653114853597"},{"version":"CommandV1","origId":2216104459508944,"guid":"7d661fed-f192-48cc-b2ae-b3ebfc874db7","subtype":"command","commandType":"auto","position":73.0,"command":"# TODO: Replace <FILL IN> with appropriate code\ndef invert(record):\n    \"\"\" Invert (ID, tokens) to a list of (token, ID)\n    Args:\n        record: a pair, (ID, token vector)\n    Returns:\n        pairs: a list of pairs of token to ID\n    \"\"\"\n    <FILL IN>\n    return (pairs)\n\namazonInvPairsRDD = (amazonWeightsRDD\n                    .<FILL IN>\n                    .cache())\n\ngoogleInvPairsRDD = (googleWeightsRDD\n                    .<FILL IN>\n                    .cache())\n\nprint 'There are %s Amazon inverted pairs and %s Google inverted pairs.' % (amazonInvPairsRDD.count(),\n                                                                            googleInvPairsRDD.count())","commandVersion":1,"state":"error","results":null,"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482178705E12,"submitTime":1.482482178705E12,"finishTime":1.482482192362E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ed24fa39-ea00-47eb-b89b-405a76566b08"},{"version":"CommandV1","origId":2216104459508945,"guid":"33e42871-bf4e-4800-9e9e-954e5bbba238","subtype":"command","commandType":"auto","position":74.0,"command":"# TEST Create inverted indicies from the full datasets (4d)\ninvertedPair = invert((1, {'foo': 2}))\nTest.assertEquals(invertedPair[0][1], 1, 'incorrect invert result')\nTest.assertEquals(amazonInvPairsRDD.count(), 111387, 'incorrect amazonInvPairsRDD.count()')\nTest.assertEquals(googleInvPairsRDD.count(), 77678, 'incorrect googleInvPairsRDD.count()')","commandVersion":1,"state":"error","results":null,"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482178716E12,"submitTime":1.482482178716E12,"finishTime":1.482482192363E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"54aa458d-5a92-4c42-9dc9-e1ae54632caa"},{"version":"CommandV1","origId":2216104459508946,"guid":"af4d24c1-43cf-4e64-8910-5a4bae5e7ee0","subtype":"command","commandType":"auto","position":75.0,"command":"%md\n#### **(4e) Identify common tokens from the full dataset**\n \nWe are now in position to efficiently perform ER on the full datasets. Implement the following algorithm to build an RDD that maps a pair of (ID, URL) to a list of tokens they share in common:\n* Using the two inverted indicies (RDDs where each element is a pair of a token and an ID or URL that contains that token), create a new RDD that contains only tokens that appear in both datasets. This will yield an RDD of pairs of (token, iterable(ID, URL)).\n* We need a mapping from (ID, URL) to token, so create a function that will swap the elements of the RDD you just created to create this new RDD consisting of ((ID, URL), token) pairs.\n* Finally, create an RDD consisting of pairs mapping (ID, URL) to all the tokens the pair shares in common","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482178726E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"eaf4b835-8f70-47df-ac75-172cb7b86ad9"},{"version":"CommandV1","origId":2216104459508947,"guid":"41d11817-1f29-40af-ab5e-772c49f60528","subtype":"command","commandType":"auto","position":76.0,"command":"# TODO: Replace <FILL IN> with appropriate code\ndef swap(record):\n    \"\"\" Swap (token, (ID, URL)) to ((ID, URL), token)\n    Args:\n        record: a pair, (token, (ID, URL))\n    Returns:\n        pair: ((ID, URL), token)\n    \"\"\"\n    token = <FILL IN>\n    keys = <FILL IN>\n    return (keys, token)\n\ncommonTokens = (amazonInvPairsRDD\n                .<FILL IN>\n                .cache())\n\nprint 'Found %d common tokens' % commonTokens.count()","commandVersion":1,"state":"error","results":null,"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482178743E12,"submitTime":1.482482178743E12,"finishTime":1.482482192364E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4ed60aba-7f40-4a2f-9210-1844cdb5eba8"},{"version":"CommandV1","origId":2216104459508948,"guid":"8089a419-2aa6-45a9-86f1-7b88c0173934","subtype":"command","commandType":"auto","position":77.0,"command":"# TEST Identify common tokens from the full dataset (4e)\nTest.assertEquals(commonTokens.count(), 2441100, 'incorrect commonTokens.count()')","commandVersion":1,"state":"error","results":null,"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482178754E12,"submitTime":1.482482178754E12,"finishTime":1.482482192365E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d0eac518-1dae-4b0c-9a18-354154397adb"},{"version":"CommandV1","origId":2216104459508949,"guid":"11f13502-c0b8-44b7-a9d0-ebdd8bac69d6","subtype":"command","commandType":"auto","position":78.0,"command":"%md\n#### **(4f) Identify common tokens from the full dataset**\n \nUse the data structures from parts **(4a)** and **(4e)** to build a dictionary to map record pairs to cosine similarity scores.\nThe steps you should perform are:\n* Create two broadcast dictionaries from the amazonWeights and googleWeights RDDs\n* Create a `fastCosinesSimilarity` function that takes in a record consisting of the pair ((Amazon ID, Google URL), tokens list) and computes the sum for each of the tokens in the token list of the products of the Amazon weight for the token times the Google weight for the token. The sum should then be divided by the norm for the Google URL and then divided by the norm for the Amazon ID. The function should return this value in a pair with the key being the (Amazon ID, Google URL). *Make sure you use broadcast variables you created for both the weights and norms*\n* Apply your `fastCosinesSimilarity` function to the common tokens from the full dataset","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482178763E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4d629703-893f-4e6d-a384-a325093813cb"},{"version":"CommandV1","origId":2216104459508950,"guid":"d47efd94-b20e-4bcc-8454-50473a5958f8","subtype":"command","commandType":"auto","position":79.0,"command":"# TODO: Replace <FILL IN> with appropriate code\namazonWeightsBroadcast = <FILL IN>\ngoogleWeightsBroadcast = <FILL IN>\n\ndef fastCosineSimilarity(record):\n    \"\"\" Compute Cosine Similarity using Broadcast variables\n    Args:\n        record: ((ID, URL), token)\n    Returns:\n        pair: ((ID, URL), cosine similarity value)\n    \"\"\"\n    amazonRec = <FILL IN>\n    googleRec = <FILL IN>\n    tokens = <FILL IN>\n    s = <FILL IN>\n    value = <FILL IN>\n    key = (amazonRec, googleRec)\n    return (key, value)\n\nsimilaritiesFullRDD = (commonTokens\n                       .<FILL IN>\n                       .cache())\n\nprint similaritiesFullRDD.count()","commandVersion":1,"state":"error","results":null,"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482178779E12,"submitTime":1.482482178779E12,"finishTime":1.482482192366E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"8e7faccb-7653-4ef9-9f35-09dda8b93e01"},{"version":"CommandV1","origId":2216104459508951,"guid":"3d685379-c000-48fa-8221-628c7c4245cf","subtype":"command","commandType":"auto","position":80.0,"command":"# TEST Identify common tokens from the full dataset (4f)\nsimilarityTest = similaritiesFullRDD.filter(lambda ((aID, gURL), cs): aID == 'b00005lzly' and gURL == 'http://www.google.com/base/feeds/snippets/13823221823254120257').collect()\nTest.assertEquals(len(similarityTest), 1, 'incorrect len(similarityTest)')\nTest.assertTrue(abs(similarityTest[0][1] - 4.286548414e-06) < 0.000000000001, 'incorrect similarityTest fastCosineSimilarity')\nTest.assertEquals(similaritiesFullRDD.count(), 2441100, 'incorrect similaritiesFullRDD.count()')","commandVersion":1,"state":"error","results":null,"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.48248217879E12,"submitTime":1.48248217879E12,"finishTime":1.482482192366E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"49898b06-7793-4400-8db7-b7613198387b"},{"version":"CommandV1","origId":2216104459508952,"guid":"dcbb6ade-517f-45f0-88d2-7e5afcfb8492","subtype":"command","commandType":"auto","position":81.0,"command":"%md\n#### **Part 5: Analysis**\n \nNow we have an authoritative list of record-pair similarities, but we need a way to use those similarities to decide if two records are duplicates or not. The simplest approach is to pick a **threshold**. Pairs whose similarity is above the threshold are declared duplicates, and pairs below the threshold are declared distinct.\n \nTo decide where to set the threshold we need to understand what kind of errors result at different levels. If we set the threshold too low, we get more **false positives**, that is, record-pairs we say are duplicates that in reality are not. If we set the threshold too high, we get more **false negatives**, that is, record-pairs that really are duplicates but that we miss.\n \nER algorithms are evaluated by the common metrics of information retrieval and search called **precision** and **recall**. Precision asks of all the record-pairs marked duplicates, what fraction are true duplicates? Recall asks of all the true duplicates in the data, what fraction did we successfully find? As with false positives and false negatives, there is a trade-off between precision and recall. A third metric, called **F-measure**, takes the harmonic mean of precision and recall to measure overall goodness in a single value:\n\\\\[ Fmeasure = 2 \\frac{precision * recall}{precision + recall} \\\\]\n \n> **Note**: In this part, we use the \"gold standard\" mapping from the included file to look up true duplicates, and the results of Part 4.\n \n> **Note**: In this part, you will not be writing any code. We've written all of the code for you. Run each cell and then answer the quiz questions on Studio.","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482178799E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"bb88feb3-e0f5-49be-aa69-34dd3ca59c74"},{"version":"CommandV1","origId":2216104459508953,"guid":"65a725a5-11de-4e04-8bbe-3fec7ba57334","subtype":"command","commandType":"auto","position":82.0,"command":"%md\n#### **(5a) Counting True Positives, False Positives, and False Negatives**\n \nWe need functions that count True Positives (true duplicates above the threshold), and False Positives and False Negatives:\n* We start with creating the `simsFullRDD` from our `similaritiesFullRDD` that consists of a pair of ((Amazon ID, Google URL), simlarity score)\n* From this RDD, we create an RDD consisting of only the similarity scores\n* To look up the similarity scores for true duplicates, we perform a left outer join using the `goldStandard` RDD and `simsFullRDD` and extract the","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482178818E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"dfc29b34-645f-49e2-a824-d696c1fca18f"},{"version":"CommandV1","origId":2216104459508954,"guid":"0fe93070-716a-4f45-b07b-006c166f0472","subtype":"command","commandType":"auto","position":83.0,"command":"# Create an RDD of ((Amazon ID, Google URL), similarity score)\nsimsFullRDD = similaritiesFullRDD.map(lambda x: (\"%s %s\" % (x[0][0], x[0][1]), x[1]))\nassert (simsFullRDD.count() == 2441100)\n\n# Create an RDD of just the similarity scores\nsimsFullValuesRDD = (simsFullRDD\n                     .map(lambda x: x[1])\n                     .cache())\nassert (simsFullValuesRDD.count() == 2441100)\n\n# Look up all similarity scores for true duplicates\n\n# This helper function will return the similarity score for records that are in the gold standard and the simsFullRDD (True positives), and will return 0 for records that are in the gold standard but not in simsFullRDD (False Negatives).\ndef gs_value(record):\n    if (record[1][1] is None):\n        return 0\n    else:\n        return record[1][1]\n\n# Join the gold standard and simsFullRDD, and then extract the similarities scores using the helper function\ntrueDupSimsRDD = (goldStandard\n                  .leftOuterJoin(simsFullRDD)\n                  .map(gs_value)\n                  .cache())\nprint 'There are %s true duplicates.' % trueDupSimsRDD.count()\nassert(trueDupSimsRDD.count() == 1300)","commandVersion":1,"state":"error","results":null,"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482178835E12,"submitTime":1.482482178835E12,"finishTime":1.482482192367E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4f287e74-b292-4a5b-bd28-77ab7ef1da12"},{"version":"CommandV1","origId":2216104459508955,"guid":"845c4637-c812-45b9-9403-d1324422c089","subtype":"command","commandType":"auto","position":84.0,"command":"%md\nThe next step is to pick a threshold between 0 and 1 for the count of True Positives (true duplicates above the threshold). However, we would like to explore many different thresholds.\n \nTo do this, we divide the space of thresholds into 100 bins, and take the following actions:\n* We use Spark Accumulators to implement our counting function. We define a custom accumulator type, `VectorAccumulatorParam`, along with functions to initialize the accumulator's vector to zero, and to add two vectors. Note that we have to use the += operator because you can only add to an accumulator.\n* We create a helper function to create a list with one entry (bit) set to a value and all others set to 0.\n* We create 101 bins for the 100 threshold values between 0 and 1.\n* Now, for each similarity score, we can compute the false positives. We do this by adding each similarity score to the appropriate bin of the vector. Then we remove true positives from the vector by using the gold standard data.\n* We define functions for computing false positive and negative and true positives, for a given threshold.","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482178845E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"08bc8049-c175-4bb2-8e8b-5ccbd5422f1d"},{"version":"CommandV1","origId":2216104459508956,"guid":"7b53ce3e-af07-43d3-8ba4-4d5b58385c3f","subtype":"command","commandType":"auto","position":85.0,"command":"from pyspark.accumulators import AccumulatorParam\nclass VectorAccumulatorParam(AccumulatorParam):\n    # Initialize the VectorAccumulator to 0\n    def zero(self, value):\n        return [0] * len(value)\n\n    # Add two VectorAccumulator variables\n    def addInPlace(self, val1, val2):\n        for i in xrange(len(val1)):\n            val1[i] += val2[i]\n        return val1\n\n# Return a list with entry x set to value and all other entries set to 0\ndef set_bit(x, value, length):\n    bits = []\n    for y in xrange(length):\n        if (x == y):\n          bits.append(value)\n        else:\n          bits.append(0)\n    return bits\n\n# Pre-bin counts of false positives for different threshold ranges\nBINS = 101\nnthresholds = 100\ndef bin(similarity):\n    return int(similarity * nthresholds)\n\n# fpCounts[i] = number of entries (possible false positives) where bin(similarity) == i\nzeros = [0] * BINS\nfpCounts = sc.accumulator(zeros, VectorAccumulatorParam())\n\ndef add_element(score):\n    global fpCounts\n    b = bin(score)\n    fpCounts += set_bit(b, 1, BINS)\n\nsimsFullValuesRDD.foreach(add_element)\n\n# Remove true positives from FP counts\ndef sub_element(score):\n    global fpCounts\n    b = bin(score)\n    fpCounts += set_bit(b, -1, BINS)\n\ntrueDupSimsRDD.foreach(sub_element)\n\ndef falsepos(threshold):\n    fpList = fpCounts.value\n    return sum([fpList[b] for b in range(0, BINS) if float(b) / nthresholds >= threshold])\n\ndef falseneg(threshold):\n    return trueDupSimsRDD.filter(lambda x: x < threshold).count()\n\ndef truepos(threshold):\n    return trueDupSimsRDD.count() - falsenegDict[threshold]\n","commandVersion":1,"state":"error","results":null,"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482178861E12,"submitTime":1.482482178861E12,"finishTime":1.482482192368E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"89f7cbaf-d426-42bb-bd7c-099a07679452"},{"version":"CommandV1","origId":2216104459508957,"guid":"d3b09a2f-7d3b-4d37-a767-fc8f86b10287","subtype":"command","commandType":"auto","position":86.0,"command":"%md\n#### **(5b) Precision, Recall, and F-measures**\nWe define functions so that we can compute the [Precision](https://en.wikipedia.org/wiki/Precision_and_recall), [Recall](https://en.wikipedia.org/wiki/Precision_and_recall), and [F-measure](https://en.wikipedia.org/wiki/Precision_and_recall#F-measure) as a function of threshold value:\n* Precision = true-positives / (true-positives + false-positives)\n* Recall = true-positives / (true-positives + false-negatives)\n* F-measure = 2 x Recall x Precision / (Recall + Precision)","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482178871E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e6a448f7-b87f-42d2-8fb1-27fd94fffe29"},{"version":"CommandV1","origId":2216104459508958,"guid":"f676ac38-76b0-4a77-9660-cccecebb8e77","subtype":"command","commandType":"auto","position":87.0,"command":"# Precision = true-positives / (true-positives + false-positives)\n# Recall = true-positives / (true-positives + false-negatives)\n# F-measure = 2 x Recall x Precision / (Recall + Precision)\n\ndef precision(threshold):\n    tp = trueposDict[threshold]\n    return float(tp) / (tp + falseposDict[threshold])\n\ndef recall(threshold):\n    tp = trueposDict[threshold]\n    return float(tp) / (tp + falsenegDict[threshold])\n\ndef fmeasure(threshold):\n    r = recall(threshold)\n    p = precision(threshold)\n    return 2 * r * p / (r + p)","commandVersion":1,"state":"error","results":null,"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482178886E12,"submitTime":1.482482178886E12,"finishTime":1.482482192369E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"fe8db877-d589-4647-9014-828b956e208f"},{"version":"CommandV1","origId":2216104459508959,"guid":"23135d17-7f87-4d63-8b0f-5a31635f307f","subtype":"command","commandType":"auto","position":88.0,"command":"%md\n#### **(5c) Line Plots**\nWe can make line plots of precision, recall, and F-measure as a function of threshold value, for thresholds between 0.0 and 1.0.  You can change `nthresholds` (above in part **(5a)**) to change the threshold values to plot.","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.482482178896E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f47cc46a-45ea-4034-906c-a3223b397594"},{"version":"CommandV1","origId":2216104459508960,"guid":"291c6e22-227a-4ec2-a414-4a4a09d9265f","subtype":"command","commandType":"auto","position":89.0,"command":"thresholds = [float(n) / nthresholds for n in range(0, nthresholds)]\nfalseposDict = dict([(t, falsepos(t)) for t in thresholds])\nfalsenegDict = dict([(t, falseneg(t)) for t in thresholds])\ntrueposDict = dict([(t, truepos(t)) for t in thresholds])\n\nprecisions = [precision(t) for t in thresholds]\nrecalls = [recall(t) for t in thresholds]\nfmeasures = [fmeasure(t) for t in thresholds]\n\nprint precisions[0], fmeasures[0]\nassert (abs(precisions[0] - 0.000532546802671) < 0.0000001)\nassert (abs(fmeasures[0] - 0.00106452669505) < 0.0000001)\n\n\nfig = plt.figure()\nplt.plot(thresholds, precisions)\nplt.plot(thresholds, recalls)\nplt.plot(thresholds, fmeasures)\nplt.legend(['Precision', 'Recall', 'F-measure'])\ndisplay(fig) \npass","commandVersion":1,"state":"error","results":null,"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.482482178911E12,"submitTime":1.482482178911E12,"finishTime":1.48248219237E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"cb1ad233-f36d-4b57-8ab9-3f62e2a7d8b0"},{"version":"CommandV1","origId":2216104459508961,"guid":"4c1a9c62-dec8-4409-83a4-8169a4977457","subtype":"command","commandType":"auto","position":90.0,"command":"\n# Create a DataFrame and visualize using display()\ngraph = [(t, precision(t), recall(t),fmeasure(t)) for t in thresholds]\ngraphRDD = sc.parallelize(graph)\n\ngraphRow = graphRDD.map(lambda (t, x, y, z): Row(threshold=t, precision=x, recall=y, fmeasure=z))\ngraphDF = sqlContext.createDataFrame(graphRow)\ndisplay(graphDF)","commandVersion":1,"state":"error","results":null,"errorSummary":"Command skipped","error":null,"workflows":[],"startTime":1.48248217892E12,"submitTime":1.48248217892E12,"finishTime":1.48248219237E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"43af5c7c-28cd-4b40-985b-6ba758cb2751"},{"version":"CommandV1","origId":2216104459508962,"guid":"09d904fb-0c03-4647-a2c5-1f3577372f10","subtype":"command","commandType":"auto","position":91.0,"command":"%md\n#### Discussion\n \nState-of-the-art tools can get an F-measure of about 60% on this dataset. In this lab exercise, our best F-measure is closer to 40%. Look at some examples of errors (both False Positives and False Negatives) and think about what went wrong.\n \n#### There are several ways we might improve our simple classifier, including:\n* Using additional attributes\n* Performing better featurization of our textual data (e.g., stemming, n-grams, etc.)\n* Using different similarity functions","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":1.48248217893E12,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e581b390-7b2c-4a39-a3ac-961953b063db"}],"dashboards":[],"guid":"9b692980-4d94-478a-9941-b8d879891867","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>
